{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frankzamma/AntiPiracyPlatform/blob/main/Notebook/PPO_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training e Test con PPO\n",
        "In questo notebook è presente il training di Q*Bert sfruttando l'algoritmo PPO"
      ],
      "metadata": {
        "id": "PcmzaRu1nh_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Repository"
      ],
      "metadata": {
        "id": "nQftMtxvnnS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "wsYUKPXjf_77"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{userdata.get('TokenGithub')}\"@github.com/amigli/Q-Bert_RL.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wIrfFpsgFyl",
        "outputId": "d65bc0f5-f0ed-404f-84ed-9b508cd7f082"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Q-Bert_RL'...\n",
            "remote: Enumerating objects: 606, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 606 (delta 46), reused 27 (delta 17), pack-reused 528 (from 1)\u001b[K\n",
            "Receiving objects: 100% (606/606), 10.49 MiB | 7.81 MiB/s, done.\n",
            "Resolving deltas: 100% (383/383), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Q-Bert_RL/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGGPMLQ3h9mQ",
        "outputId": "a84e5385-f191-4efd-e709-a16c89f72de3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Q-Bert_RL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installazione dei requirements"
      ],
      "metadata": {
        "id": "_-684nkWjBiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc262078-97db-48e0-d5b1-8ca34da07987",
        "id": "vHVbipD0n6bi"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ale-py"
      ],
      "metadata": {
        "id": "ipM5179yjT1z",
        "outputId": "f7f1a11f-54ce-407f-e4e3-49a627eb9898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.11/dist-packages (from ale-py) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "I8QkWBD2jTt1",
        "outputId": "d49659f1-a594-40c8-b830-788667d251e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.36.1)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "V5BpWXPAjZDx",
        "outputId": "42971644-855e-46a0-8e8c-a07d154d1173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LklXPoeT3Wlh",
        "outputId": "1316e85a-93c3-4b17-ccaf-fd71a6bf51a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo"
      ],
      "metadata": {
        "id": "nh4inZutVC6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import ale_py\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from stable_baselines3.common.logger import configure\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "from EnvironmentWrappers.RewardFunction import RewardFunction\n",
        "from EnvironmentWrappers.ObsRewardWrapper import ObsRewardWrapper\n",
        "import wandb\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.utils import get_linear_fn\n",
        "import torch as th\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from stable_baselines3.common.atari_wrappers import NoopResetEnv\n"
      ],
      "metadata": {
        "id": "JfZKv1fAVE6_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wandb"
      ],
      "metadata": {
        "id": "cFm2KbYv3fXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwP3p9wV3gXP",
        "outputId": "73ee35d6-74dd-49bb-e6ae-51e144fb432f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"policy_type\": \"MlpPolicy\",\n",
        "    \"total_timesteps\": 7_000_000,\n",
        "    \"env_name\": \"ALE/Qbert-ram-v5\",\n",
        "}\n",
        "run = wandb.init(\n",
        "    project=\"QBERT-RL\",\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
        "    monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    save_code=True,  # optional\n",
        "    entity=\"Q-BertRLTeam\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "ZjfXxkuV3iBJ",
        "outputId": "4bccdaf2-b82c-4a0a-94e8-8b6b2cc44a9b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  return LooseVersion(v) >= LooseVersion(check)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrank581-fgz\u001b[0m (\u001b[33mfrankzamma\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/Q-Bert_RL/wandb/run-20250201_200015-vx73kc3x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/Q-BertRLTeam/QBERT-RL/runs/vx73kc3x' target=\"_blank\">jolly-resonance-14</a></strong> to <a href='https://wandb.ai/Q-BertRLTeam/QBERT-RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/Q-BertRLTeam/QBERT-RL' target=\"_blank\">https://wandb.ai/Q-BertRLTeam/QBERT-RL</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/Q-BertRLTeam/QBERT-RL/runs/vx73kc3x' target=\"_blank\">https://wandb.ai/Q-BertRLTeam/QBERT-RL/runs/vx73kc3x</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "86Qu_73Xdmz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gym.register_envs(ale_py)\n",
        "\n",
        "def make_env(env_id):\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        env = ObsRewardWrapper(env)\n",
        "        env = NoopResetEnv(env, noop_max=30)\n",
        "        env = Monitor(env)\n",
        "        # env = OurRewardWrapper(env)\n",
        "        return env\n",
        "    return _init"
      ],
      "metadata": {
        "id": "gUCghEndXpvK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
        "                     net_arch=dict(pi=[256, 512, 256, 128, 64], vi=[256, 512, 256, 128, 64]))\n"
      ],
      "metadata": {
        "id": "drxuajRhp7jg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "\n",
        "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
        "    \"\"\"\n",
        "    Linear learning rate schedule.\n",
        "\n",
        "    :param initial_value: Initial learning rate.\n",
        "    :return: schedule that computes\n",
        "      current learning rate depending on remaining progress\n",
        "    \"\"\"\n",
        "    def func(progress_remaining: float) -> float:\n",
        "        \"\"\"\n",
        "        Progress will decrease from 1 (beginning) to 0.\n",
        "\n",
        "        :param progress_remaining:\n",
        "        :return: current learning rate\n",
        "        \"\"\"\n",
        "        return progress_remaining * initial_value\n",
        "\n",
        "    return func"
      ],
      "metadata": {
        "id": "tRQV-8tnuRTw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = linear_schedule(0.0001)\n",
        "\n",
        "num_envs = 25\n",
        "envs = DummyVecEnv([make_env(\"ALE/Qbert-ram-v5\") for _ in range(num_envs)])\n",
        "\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    envs,\n",
        "    verbose=1,\n",
        "    n_steps = 1024,\n",
        "    batch_size=64,\n",
        "    ent_coef= 0.03,\n",
        "    learning_rate=lr_schedule,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    tensorboard_log=f\"runs/{run.id}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsryg8CAVEHu",
        "outputId": "d2d16fde-fde0-421a-a947-6aa509c04cb0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"ALE/Qbert-ram-v5\")\n",
        "env = ObsRewardWrapper(env)\n",
        "\n",
        "eval_callback = EvalCallback(env, best_model_save_path=\"./BestModels/\",\n",
        "                             log_path=\"./logs/\", eval_freq=250,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "wandb_callback = WandbCallback(\n",
        "        gradient_save_freq=500,\n",
        "        model_save_path=f\"models/{run.id}\",\n",
        "        verbose=2,\n",
        "    )\n",
        "\n",
        "model.learn(total_timesteps = 7_000_000, callback=[wandb_callback,eval_callback])\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "# Valutazione del modello\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Ricompensa media: {mean_reward:.2f}, deviazione standard: {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "ImI77TRtXtcx",
        "outputId": "1082e887-934c-4563-fab5-e7fdd12579ec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging to runs/vx73kc3x/PPO_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "|    explained_variance   | 0.84        |\n",
            "|    learning_rate        | 5.83e-05    |\n",
            "|    loss                 | 0.21        |\n",
            "|    n_updates            | 1140        |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 1.5         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=2925000, episode_reward=8.80 +/- 8.35\n",
            "Episode length: 812.80 +/- 191.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 813      |\n",
            "|    mean_reward     | 8.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2925000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2931250, episode_reward=12.20 +/- 8.03\n",
            "Episode length: 917.80 +/- 148.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 918      |\n",
            "|    mean_reward     | 12.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2931250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2937500, episode_reward=6.80 +/- 9.06\n",
            "Episode length: 849.60 +/- 154.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 850      |\n",
            "|    mean_reward     | 6.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2937500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2943750, episode_reward=9.80 +/- 5.31\n",
            "Episode length: 810.80 +/- 113.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 811      |\n",
            "|    mean_reward     | 9.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2943750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 448      |\n",
            "|    ep_rew_mean     | 5.83     |\n",
            "| time/              |          |\n",
            "|    fps             | 378      |\n",
            "|    iterations      | 115      |\n",
            "|    time_elapsed    | 7781     |\n",
            "|    total_timesteps | 2944000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2950000, episode_reward=2.80 +/- 1.47\n",
            "Episode length: 732.00 +/- 44.54\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 732         |\n",
            "|    mean_reward          | 2.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 2950000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015580955 |\n",
            "|    clip_fraction        | 0.183       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.53       |\n",
            "|    explained_variance   | 0.861       |\n",
            "|    learning_rate        | 5.79e-05    |\n",
            "|    loss                 | 0.179       |\n",
            "|    n_updates            | 1150        |\n",
            "|    policy_gradient_loss | -0.0144     |\n",
            "|    value_loss           | 1.49        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=2956250, episode_reward=5.00 +/- 3.29\n",
            "Episode length: 729.20 +/- 42.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 729      |\n",
            "|    mean_reward     | 5        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2956250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2962500, episode_reward=5.40 +/- 3.20\n",
            "Episode length: 729.20 +/- 42.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 729      |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2962500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2968750, episode_reward=6.80 +/- 3.60\n",
            "Episode length: 732.40 +/- 43.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 732      |\n",
            "|    mean_reward     | 6.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2968750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 458      |\n",
            "|    ep_rew_mean     | 5.84     |\n",
            "| time/              |          |\n",
            "|    fps             | 378      |\n",
            "|    iterations      | 116      |\n",
            "|    time_elapsed    | 7850     |\n",
            "|    total_timesteps | 2969600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2975000, episode_reward=6.80 +/- 5.60\n",
            "Episode length: 708.20 +/- 75.60\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 708         |\n",
            "|    mean_reward          | 6.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 2975000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014813328 |\n",
            "|    clip_fraction        | 0.172       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.844       |\n",
            "|    learning_rate        | 5.76e-05    |\n",
            "|    loss                 | 1.58        |\n",
            "|    n_updates            | 1160        |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    value_loss           | 1.63        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=2981250, episode_reward=8.40 +/- 4.96\n",
            "Episode length: 713.00 +/- 74.26\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 713      |\n",
            "|    mean_reward     | 8.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2981250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2987500, episode_reward=7.30 +/- 6.60\n",
            "Episode length: 758.80 +/- 25.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 759      |\n",
            "|    mean_reward     | 7.3      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2987500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2993750, episode_reward=7.80 +/- 5.49\n",
            "Episode length: 699.40 +/- 73.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 699      |\n",
            "|    mean_reward     | 7.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2993750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 460      |\n",
            "|    ep_rew_mean     | 5.66     |\n",
            "| time/              |          |\n",
            "|    fps             | 378      |\n",
            "|    iterations      | 117      |\n",
            "|    time_elapsed    | 7919     |\n",
            "|    total_timesteps | 2995200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3000000, episode_reward=8.50 +/- 2.86\n",
            "Episode length: 796.20 +/- 317.81\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 796          |\n",
            "|    mean_reward          | 8.5          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 3000000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0154304765 |\n",
            "|    clip_fraction        | 0.174        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.51        |\n",
            "|    explained_variance   | 0.869        |\n",
            "|    learning_rate        | 5.72e-05     |\n",
            "|    loss                 | 0.435        |\n",
            "|    n_updates            | 1170         |\n",
            "|    policy_gradient_loss | -0.0139      |\n",
            "|    value_loss           | 1.42         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=3006250, episode_reward=7.50 +/- 2.68\n",
            "Episode length: 736.80 +/- 310.87\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 737      |\n",
            "|    mean_reward     | 7.5      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3006250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3012500, episode_reward=5.20 +/- 5.38\n",
            "Episode length: 806.80 +/- 171.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 807      |\n",
            "|    mean_reward     | 5.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3012500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3018750, episode_reward=9.60 +/- 5.77\n",
            "Episode length: 922.40 +/- 493.74\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 922      |\n",
            "|    mean_reward     | 9.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3018750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 464      |\n",
            "|    ep_rew_mean     | 6.87     |\n",
            "| time/              |          |\n",
            "|    fps             | 377      |\n",
            "|    iterations      | 118      |\n",
            "|    time_elapsed    | 7991     |\n",
            "|    total_timesteps | 3020800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3025000, episode_reward=6.20 +/- 5.89\n",
            "Episode length: 964.40 +/- 103.64\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 964         |\n",
            "|    mean_reward          | 6.2         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3025000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014855266 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.888       |\n",
            "|    learning_rate        | 5.68e-05    |\n",
            "|    loss                 | 0.154       |\n",
            "|    n_updates            | 1180        |\n",
            "|    policy_gradient_loss | -0.0137     |\n",
            "|    value_loss           | 1.24        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3031250, episode_reward=8.00 +/- 10.63\n",
            "Episode length: 1006.60 +/- 21.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.01e+03 |\n",
            "|    mean_reward     | 8        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3031250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3037500, episode_reward=16.70 +/- 5.08\n",
            "Episode length: 1036.60 +/- 6.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 16.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3037500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3043750, episode_reward=12.10 +/- 8.28\n",
            "Episode length: 1018.20 +/- 20.87\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 12.1     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3043750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 476      |\n",
            "|    ep_rew_mean     | 7.24     |\n",
            "| time/              |          |\n",
            "|    fps             | 377      |\n",
            "|    iterations      | 119      |\n",
            "|    time_elapsed    | 8072     |\n",
            "|    total_timesteps | 3046400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3050000, episode_reward=-1.00 +/- 0.00\n",
            "Episode length: 640.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 640        |\n",
            "|    mean_reward          | -1         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 3050000    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01516931 |\n",
            "|    clip_fraction        | 0.166      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.5       |\n",
            "|    explained_variance   | 0.875      |\n",
            "|    learning_rate        | 5.65e-05   |\n",
            "|    loss                 | 1.34       |\n",
            "|    n_updates            | 1190       |\n",
            "|    policy_gradient_loss | -0.0137    |\n",
            "|    value_loss           | 1.37       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=3056250, episode_reward=2.40 +/- 6.31\n",
            "Episode length: 689.40 +/- 89.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 689      |\n",
            "|    mean_reward     | 2.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3056250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3062500, episode_reward=-1.00 +/- 0.00\n",
            "Episode length: 671.00 +/- 62.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 671      |\n",
            "|    mean_reward     | -1       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3062500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3068750, episode_reward=2.20 +/- 6.40\n",
            "Episode length: 658.40 +/- 72.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 658      |\n",
            "|    mean_reward     | 2.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3068750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 461      |\n",
            "|    ep_rew_mean     | 6.59     |\n",
            "| time/              |          |\n",
            "|    fps             | 377      |\n",
            "|    iterations      | 120      |\n",
            "|    time_elapsed    | 8139     |\n",
            "|    total_timesteps | 3072000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3075000, episode_reward=9.80 +/- 6.37\n",
            "Episode length: 882.40 +/- 72.52\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 882         |\n",
            "|    mean_reward          | 9.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3075000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014024808 |\n",
            "|    clip_fraction        | 0.156       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.51       |\n",
            "|    explained_variance   | 0.859       |\n",
            "|    learning_rate        | 5.61e-05    |\n",
            "|    loss                 | 0.346       |\n",
            "|    n_updates            | 1200        |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    value_loss           | 1.41        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3081250, episode_reward=5.60 +/- 6.71\n",
            "Episode length: 879.20 +/- 102.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 879      |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3081250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3087500, episode_reward=6.40 +/- 5.24\n",
            "Episode length: 813.20 +/- 189.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 813      |\n",
            "|    mean_reward     | 6.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3087500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3093750, episode_reward=9.40 +/- 2.73\n",
            "Episode length: 837.00 +/- 48.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 837      |\n",
            "|    mean_reward     | 9.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3093750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 456      |\n",
            "|    ep_rew_mean     | 6.54     |\n",
            "| time/              |          |\n",
            "|    fps             | 377      |\n",
            "|    iterations      | 121      |\n",
            "|    time_elapsed    | 8212     |\n",
            "|    total_timesteps | 3097600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3100000, episode_reward=9.00 +/- 4.05\n",
            "Episode length: 768.20 +/- 63.98\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 768         |\n",
            "|    mean_reward          | 9           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3100000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014939244 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.883       |\n",
            "|    learning_rate        | 5.57e-05    |\n",
            "|    loss                 | 0.247       |\n",
            "|    n_updates            | 1210        |\n",
            "|    policy_gradient_loss | -0.0128     |\n",
            "|    value_loss           | 1.38        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3106250, episode_reward=10.60 +/- 0.49\n",
            "Episode length: 781.60 +/- 24.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 782      |\n",
            "|    mean_reward     | 10.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3106250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3112500, episode_reward=8.80 +/- 3.92\n",
            "Episode length: 748.60 +/- 54.70\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 749      |\n",
            "|    mean_reward     | 8.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3112500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3118750, episode_reward=6.80 +/- 4.75\n",
            "Episode length: 719.60 +/- 96.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 720      |\n",
            "|    mean_reward     | 6.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3118750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 478      |\n",
            "|    ep_rew_mean     | 7.08     |\n",
            "| time/              |          |\n",
            "|    fps             | 377      |\n",
            "|    iterations      | 122      |\n",
            "|    time_elapsed    | 8283     |\n",
            "|    total_timesteps | 3123200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3125000, episode_reward=13.20 +/- 0.40\n",
            "Episode length: 795.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 795         |\n",
            "|    mean_reward          | 13.2        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3125000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015193059 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.884       |\n",
            "|    learning_rate        | 5.54e-05    |\n",
            "|    loss                 | 0.59        |\n",
            "|    n_updates            | 1220        |\n",
            "|    policy_gradient_loss | -0.0143     |\n",
            "|    value_loss           | 1.23        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3131250, episode_reward=13.40 +/- 0.49\n",
            "Episode length: 795.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 795      |\n",
            "|    mean_reward     | 13.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3131250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3137500, episode_reward=13.30 +/- 7.70\n",
            "Episode length: 781.80 +/- 71.68\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 782      |\n",
            "|    mean_reward     | 13.3     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3137500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3143750, episode_reward=9.80 +/- 4.92\n",
            "Episode length: 758.60 +/- 47.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 759      |\n",
            "|    mean_reward     | 9.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3143750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 477      |\n",
            "|    ep_rew_mean     | 7.06     |\n",
            "| time/              |          |\n",
            "|    fps             | 376      |\n",
            "|    iterations      | 123      |\n",
            "|    time_elapsed    | 8354     |\n",
            "|    total_timesteps | 3148800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3150000, episode_reward=10.00 +/- 4.60\n",
            "Episode length: 752.00 +/- 55.08\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 752        |\n",
            "|    mean_reward          | 10         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 3150000    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01425164 |\n",
            "|    clip_fraction        | 0.17       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.48      |\n",
            "|    explained_variance   | 0.88       |\n",
            "|    learning_rate        | 5.5e-05    |\n",
            "|    loss                 | 0.526      |\n",
            "|    n_updates            | 1230       |\n",
            "|    policy_gradient_loss | -0.0133    |\n",
            "|    value_loss           | 1.39       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=3156250, episode_reward=9.90 +/- 6.67\n",
            "Episode length: 762.80 +/- 94.29\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 763      |\n",
            "|    mean_reward     | 9.9      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3156250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3162500, episode_reward=13.20 +/- 0.40\n",
            "Episode length: 795.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 795      |\n",
            "|    mean_reward     | 13.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3162500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3168750, episode_reward=11.20 +/- 4.12\n",
            "Episode length: 768.40 +/- 53.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 768      |\n",
            "|    mean_reward     | 11.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3168750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 464      |\n",
            "|    ep_rew_mean     | 5.98     |\n",
            "| time/              |          |\n",
            "|    fps             | 376      |\n",
            "|    iterations      | 124      |\n",
            "|    time_elapsed    | 8425     |\n",
            "|    total_timesteps | 3174400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3175000, episode_reward=4.20 +/- 1.17\n",
            "Episode length: 755.20 +/- 98.78\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 755         |\n",
            "|    mean_reward          | 4.2         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3175000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014742855 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.855       |\n",
            "|    learning_rate        | 5.47e-05    |\n",
            "|    loss                 | 0.252       |\n",
            "|    n_updates            | 1240        |\n",
            "|    policy_gradient_loss | -0.0124     |\n",
            "|    value_loss           | 1.58        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3181250, episode_reward=4.20 +/- 1.33\n",
            "Episode length: 771.40 +/- 88.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 771      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3181250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3187500, episode_reward=5.00 +/- 0.63\n",
            "Episode length: 785.00 +/- 78.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 785      |\n",
            "|    mean_reward     | 5        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3187500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3193750, episode_reward=4.20 +/- 1.17\n",
            "Episode length: 732.40 +/- 27.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 732      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3193750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3200000, episode_reward=4.20 +/- 1.47\n",
            "Episode length: 757.80 +/- 96.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 758      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3200000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 464      |\n",
            "|    ep_rew_mean     | 7.01     |\n",
            "| time/              |          |\n",
            "|    fps             | 376      |\n",
            "|    iterations      | 125      |\n",
            "|    time_elapsed    | 8503     |\n",
            "|    total_timesteps | 3200000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3206250, episode_reward=6.80 +/- 6.34\n",
            "Episode length: 730.40 +/- 82.06\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 730         |\n",
            "|    mean_reward          | 6.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3206250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015384071 |\n",
            "|    clip_fraction        | 0.178       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.873       |\n",
            "|    learning_rate        | 5.43e-05    |\n",
            "|    loss                 | 0.133       |\n",
            "|    n_updates            | 1250        |\n",
            "|    policy_gradient_loss | -0.0135     |\n",
            "|    value_loss           | 1.29        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3212500, episode_reward=14.40 +/- 0.49\n",
            "Episode length: 746.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 746      |\n",
            "|    mean_reward     | 14.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3212500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3218750, episode_reward=5.60 +/- 6.95\n",
            "Episode length: 692.40 +/- 45.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 692      |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3218750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3225000, episode_reward=9.20 +/- 5.91\n",
            "Episode length: 750.40 +/- 70.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 750      |\n",
            "|    mean_reward     | 9.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3225000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 471      |\n",
            "|    ep_rew_mean     | 7.49     |\n",
            "| time/              |          |\n",
            "|    fps             | 376      |\n",
            "|    iterations      | 126      |\n",
            "|    time_elapsed    | 8573     |\n",
            "|    total_timesteps | 3225600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3231250, episode_reward=13.60 +/- 1.85\n",
            "Episode length: 752.40 +/- 12.80\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 752         |\n",
            "|    mean_reward          | 13.6        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3231250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014182262 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.88        |\n",
            "|    learning_rate        | 5.39e-05    |\n",
            "|    loss                 | 0.242       |\n",
            "|    n_updates            | 1260        |\n",
            "|    policy_gradient_loss | -0.0137     |\n",
            "|    value_loss           | 1.36        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3237500, episode_reward=9.40 +/- 3.56\n",
            "Episode length: 750.00 +/- 41.88\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 750      |\n",
            "|    mean_reward     | 9.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3237500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3243750, episode_reward=12.00 +/- 4.52\n",
            "Episode length: 762.20 +/- 32.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 762      |\n",
            "|    mean_reward     | 12       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3243750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3250000, episode_reward=13.60 +/- 1.85\n",
            "Episode length: 759.00 +/- 26.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 759      |\n",
            "|    mean_reward     | 13.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3250000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 474      |\n",
            "|    ep_rew_mean     | 6.89     |\n",
            "| time/              |          |\n",
            "|    fps             | 376      |\n",
            "|    iterations      | 127      |\n",
            "|    time_elapsed    | 8643     |\n",
            "|    total_timesteps | 3251200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3256250, episode_reward=4.40 +/- 4.45\n",
            "Episode length: 742.60 +/- 16.04\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 743         |\n",
            "|    mean_reward          | 4.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3256250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016381957 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 5.36e-05    |\n",
            "|    loss                 | 0.381       |\n",
            "|    n_updates            | 1270        |\n",
            "|    policy_gradient_loss | -0.0117     |\n",
            "|    value_loss           | 1.36        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3262500, episode_reward=0.20 +/- 3.54\n",
            "Episode length: 726.20 +/- 16.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 726      |\n",
            "|    mean_reward     | 0.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3262500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3268750, episode_reward=2.80 +/- 2.93\n",
            "Episode length: 739.40 +/- 13.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 739      |\n",
            "|    mean_reward     | 2.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3268750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3275000, episode_reward=3.00 +/- 3.03\n",
            "Episode length: 739.40 +/- 13.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 739      |\n",
            "|    mean_reward     | 3        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3275000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 480      |\n",
            "|    ep_rew_mean     | 7.48     |\n",
            "| time/              |          |\n",
            "|    fps             | 376      |\n",
            "|    iterations      | 128      |\n",
            "|    time_elapsed    | 8713     |\n",
            "|    total_timesteps | 3276800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3281250, episode_reward=5.60 +/- 7.66\n",
            "Episode length: 732.80 +/- 16.17\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 733         |\n",
            "|    mean_reward          | 5.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3281250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014628034 |\n",
            "|    clip_fraction        | 0.181       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.882       |\n",
            "|    learning_rate        | 5.32e-05    |\n",
            "|    loss                 | 0.694       |\n",
            "|    n_updates            | 1280        |\n",
            "|    policy_gradient_loss | -0.0141     |\n",
            "|    value_loss           | 1.31        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3287500, episode_reward=4.20 +/- 8.82\n",
            "Episode length: 726.20 +/- 16.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 726      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3287500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3293750, episode_reward=11.50 +/- 6.02\n",
            "Episode length: 794.00 +/- 96.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 794      |\n",
            "|    mean_reward     | 11.5     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3293750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3300000, episode_reward=10.80 +/- 6.91\n",
            "Episode length: 739.40 +/- 13.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 739      |\n",
            "|    mean_reward     | 10.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3300000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 481      |\n",
            "|    ep_rew_mean     | 7.5      |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 129      |\n",
            "|    time_elapsed    | 8783     |\n",
            "|    total_timesteps | 3302400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3306250, episode_reward=5.00 +/- 6.39\n",
            "Episode length: 752.80 +/- 22.47\n",
            "---------------------------------------\n",
            "| eval/                   |           |\n",
            "|    mean_ep_length       | 753       |\n",
            "|    mean_reward          | 5         |\n",
            "| time/                   |           |\n",
            "|    total_timesteps      | 3306250   |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0154167 |\n",
            "|    clip_fraction        | 0.165     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.44     |\n",
            "|    explained_variance   | 0.883     |\n",
            "|    learning_rate        | 5.28e-05  |\n",
            "|    loss                 | 0.432     |\n",
            "|    n_updates            | 1290      |\n",
            "|    policy_gradient_loss | -0.0138   |\n",
            "|    value_loss           | 1.32      |\n",
            "---------------------------------------\n",
            "Eval num_timesteps=3312500, episode_reward=0.00 +/- 8.00\n",
            "Episode length: 743.20 +/- 26.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 743      |\n",
            "|    mean_reward     | 0        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3312500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3318750, episode_reward=2.60 +/- 3.32\n",
            "Episode length: 742.80 +/- 6.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 743      |\n",
            "|    mean_reward     | 2.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3318750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3325000, episode_reward=4.40 +/- 0.49\n",
            "Episode length: 746.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 746      |\n",
            "|    mean_reward     | 4.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3325000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 483      |\n",
            "|    ep_rew_mean     | 6.98     |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 130      |\n",
            "|    time_elapsed    | 8853     |\n",
            "|    total_timesteps | 3328000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3331250, episode_reward=4.00 +/- 0.63\n",
            "Episode length: 742.80 +/- 6.40\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 743          |\n",
            "|    mean_reward          | 4            |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 3331250      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0149589395 |\n",
            "|    clip_fraction        | 0.159        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.857        |\n",
            "|    learning_rate        | 5.25e-05     |\n",
            "|    loss                 | 0.569        |\n",
            "|    n_updates            | 1300         |\n",
            "|    policy_gradient_loss | -0.0124      |\n",
            "|    value_loss           | 1.56         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=3337500, episode_reward=2.60 +/- 2.24\n",
            "Episode length: 702.80 +/- 46.74\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 703      |\n",
            "|    mean_reward     | 2.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3337500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3343750, episode_reward=2.60 +/- 2.15\n",
            "Episode length: 706.00 +/- 48.99\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 706      |\n",
            "|    mean_reward     | 2.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3343750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3350000, episode_reward=3.80 +/- 0.40\n",
            "Episode length: 742.80 +/- 6.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 743      |\n",
            "|    mean_reward     | 3.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3350000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 476      |\n",
            "|    ep_rew_mean     | 6.47     |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 131      |\n",
            "|    time_elapsed    | 8921     |\n",
            "|    total_timesteps | 3353600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3356250, episode_reward=-1.20 +/- 4.66\n",
            "Episode length: 763.80 +/- 42.39\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 764          |\n",
            "|    mean_reward          | -1.2         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 3356250      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0148614235 |\n",
            "|    clip_fraction        | 0.165        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.891        |\n",
            "|    learning_rate        | 5.21e-05     |\n",
            "|    loss                 | 0.295        |\n",
            "|    n_updates            | 1310         |\n",
            "|    policy_gradient_loss | -0.0108      |\n",
            "|    value_loss           | 1.28         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=3362500, episode_reward=2.60 +/- 3.83\n",
            "Episode length: 744.20 +/- 33.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 744      |\n",
            "|    mean_reward     | 2.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3362500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3368750, episode_reward=2.20 +/- 3.60\n",
            "Episode length: 755.80 +/- 19.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 756      |\n",
            "|    mean_reward     | 2.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3368750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3375000, episode_reward=-1.40 +/- 4.41\n",
            "Episode length: 775.40 +/- 24.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 775      |\n",
            "|    mean_reward     | -1.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3375000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 488      |\n",
            "|    ep_rew_mean     | 6.79     |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 132      |\n",
            "|    time_elapsed    | 8991     |\n",
            "|    total_timesteps | 3379200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3381250, episode_reward=2.80 +/- 2.93\n",
            "Episode length: 739.40 +/- 13.20\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 739         |\n",
            "|    mean_reward          | 2.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3381250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015642429 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.44       |\n",
            "|    explained_variance   | 0.861       |\n",
            "|    learning_rate        | 5.17e-05    |\n",
            "|    loss                 | 0.364       |\n",
            "|    n_updates            | 1320        |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    value_loss           | 1.54        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3387500, episode_reward=3.00 +/- 3.03\n",
            "Episode length: 739.40 +/- 13.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 739      |\n",
            "|    mean_reward     | 3        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3387500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3393750, episode_reward=4.40 +/- 0.49\n",
            "Episode length: 788.00 +/- 51.44\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 788      |\n",
            "|    mean_reward     | 4.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3393750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3400000, episode_reward=2.80 +/- 2.93\n",
            "Episode length: 739.40 +/- 13.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 739      |\n",
            "|    mean_reward     | 2.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3400000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 486      |\n",
            "|    ep_rew_mean     | 7.13     |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 133      |\n",
            "|    time_elapsed    | 9063     |\n",
            "|    total_timesteps | 3404800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3406250, episode_reward=0.20 +/- 3.12\n",
            "Episode length: 726.20 +/- 16.17\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 726         |\n",
            "|    mean_reward          | 0.2         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3406250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015542869 |\n",
            "|    clip_fraction        | 0.175       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.883       |\n",
            "|    learning_rate        | 5.14e-05    |\n",
            "|    loss                 | 0.236       |\n",
            "|    n_updates            | 1330        |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 1.2         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3412500, episode_reward=6.00 +/- 2.53\n",
            "Episode length: 749.20 +/- 6.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 749      |\n",
            "|    mean_reward     | 6        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3412500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3418750, episode_reward=6.20 +/- 4.07\n",
            "Episode length: 775.20 +/- 43.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 775      |\n",
            "|    mean_reward     | 6.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3418750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3425000, episode_reward=4.40 +/- 5.82\n",
            "Episode length: 739.20 +/- 22.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 739      |\n",
            "|    mean_reward     | 4.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3425000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 491      |\n",
            "|    ep_rew_mean     | 8.16     |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 134      |\n",
            "|    time_elapsed    | 9135     |\n",
            "|    total_timesteps | 3430400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3431250, episode_reward=12.20 +/- 5.88\n",
            "Episode length: 628.40 +/- 96.02\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 628         |\n",
            "|    mean_reward          | 12.2        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3431250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014710619 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.901       |\n",
            "|    learning_rate        | 5.1e-05     |\n",
            "|    loss                 | 0.684       |\n",
            "|    n_updates            | 1340        |\n",
            "|    policy_gradient_loss | -0.0124     |\n",
            "|    value_loss           | 1.16        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3437500, episode_reward=8.80 +/- 6.73\n",
            "Episode length: 699.20 +/- 145.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 699      |\n",
            "|    mean_reward     | 8.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3437500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3443750, episode_reward=8.80 +/- 6.73\n",
            "Episode length: 600.80 +/- 128.82\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 601      |\n",
            "|    mean_reward     | 8.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3443750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3450000, episode_reward=7.00 +/- 5.02\n",
            "Episode length: 756.00 +/- 20.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 756      |\n",
            "|    mean_reward     | 7        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3450000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 504      |\n",
            "|    ep_rew_mean     | 8.15     |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 135      |\n",
            "|    time_elapsed    | 9206     |\n",
            "|    total_timesteps | 3456000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3456250, episode_reward=2.80 +/- 3.43\n",
            "Episode length: 739.40 +/- 13.20\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 739        |\n",
            "|    mean_reward          | 2.8        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 3456250    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01403906 |\n",
            "|    clip_fraction        | 0.161      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.45      |\n",
            "|    explained_variance   | 0.896      |\n",
            "|    learning_rate        | 5.06e-05   |\n",
            "|    loss                 | 1.23       |\n",
            "|    n_updates            | 1350       |\n",
            "|    policy_gradient_loss | -0.012     |\n",
            "|    value_loss           | 1.26       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=3462500, episode_reward=1.20 +/- 3.49\n",
            "Episode length: 687.40 +/- 72.35\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 687      |\n",
            "|    mean_reward     | 1.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3462500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3468750, episode_reward=5.00 +/- 0.00\n",
            "Episode length: 698.00 +/- 39.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 698      |\n",
            "|    mean_reward     | 5        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3468750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3475000, episode_reward=4.20 +/- 0.40\n",
            "Episode length: 730.00 +/- 32.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 730      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3475000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3481250, episode_reward=4.40 +/- 0.49\n",
            "Episode length: 730.00 +/- 32.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 730      |\n",
            "|    mean_reward     | 4.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3481250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 482      |\n",
            "|    ep_rew_mean     | 6.71     |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 136      |\n",
            "|    time_elapsed    | 9282     |\n",
            "|    total_timesteps | 3481600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3487500, episode_reward=6.20 +/- 2.40\n",
            "Episode length: 931.20 +/- 130.93\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 931         |\n",
            "|    mean_reward          | 6.2         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3487500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013743059 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.853       |\n",
            "|    learning_rate        | 5.03e-05    |\n",
            "|    loss                 | 0.958       |\n",
            "|    n_updates            | 1360        |\n",
            "|    policy_gradient_loss | -0.0119     |\n",
            "|    value_loss           | 1.62        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3493750, episode_reward=5.20 +/- 0.40\n",
            "Episode length: 986.40 +/- 103.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 986      |\n",
            "|    mean_reward     | 5.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3493750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3500000, episode_reward=6.20 +/- 2.40\n",
            "Episode length: 931.20 +/- 130.93\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 931      |\n",
            "|    mean_reward     | 6.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3500000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3506250, episode_reward=5.60 +/- 2.94\n",
            "Episode length: 788.40 +/- 145.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 788      |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3506250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 467      |\n",
            "|    ep_rew_mean     | 5.74     |\n",
            "| time/              |          |\n",
            "|    fps             | 374      |\n",
            "|    iterations      | 137      |\n",
            "|    time_elapsed    | 9358     |\n",
            "|    total_timesteps | 3507200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3512500, episode_reward=6.20 +/- 5.31\n",
            "Episode length: 789.20 +/- 206.23\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 789         |\n",
            "|    mean_reward          | 6.2         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3512500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015358138 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.843       |\n",
            "|    learning_rate        | 4.99e-05    |\n",
            "|    loss                 | 0.606       |\n",
            "|    n_updates            | 1370        |\n",
            "|    policy_gradient_loss | -0.0146     |\n",
            "|    value_loss           | 1.6         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3518750, episode_reward=6.00 +/- 0.00\n",
            "Episode length: 967.40 +/- 141.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 967      |\n",
            "|    mean_reward     | 6        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3518750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3525000, episode_reward=7.60 +/- 3.72\n",
            "Episode length: 855.80 +/- 223.16\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 856      |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3525000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3531250, episode_reward=5.40 +/- 0.49\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3531250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 474      |\n",
            "|    ep_rew_mean     | 6.34     |\n",
            "| time/              |          |\n",
            "|    fps             | 374      |\n",
            "|    iterations      | 138      |\n",
            "|    time_elapsed    | 9436     |\n",
            "|    total_timesteps | 3532800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3537500, episode_reward=3.60 +/- 2.24\n",
            "Episode length: 788.20 +/- 218.16\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 788         |\n",
            "|    mean_reward          | 3.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3537500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013894789 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 4.95e-05    |\n",
            "|    loss                 | 1.09        |\n",
            "|    n_updates            | 1380        |\n",
            "|    policy_gradient_loss | -0.0138     |\n",
            "|    value_loss           | 1.41        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3543750, episode_reward=3.40 +/- 2.06\n",
            "Episode length: 788.20 +/- 218.16\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 788      |\n",
            "|    mean_reward     | 3.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3543750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3550000, episode_reward=4.20 +/- 0.40\n",
            "Episode length: 818.00 +/- 110.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 818      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3550000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3556250, episode_reward=5.50 +/- 2.53\n",
            "Episode length: 840.80 +/- 108.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 841      |\n",
            "|    mean_reward     | 5.5      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3556250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 487      |\n",
            "|    ep_rew_mean     | 7.13     |\n",
            "| time/              |          |\n",
            "|    fps             | 374      |\n",
            "|    iterations      | 139      |\n",
            "|    time_elapsed    | 9509     |\n",
            "|    total_timesteps | 3558400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3562500, episode_reward=4.80 +/- 2.23\n",
            "Episode length: 863.00 +/- 193.68\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 863         |\n",
            "|    mean_reward          | 4.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3562500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015431686 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.878       |\n",
            "|    learning_rate        | 4.92e-05    |\n",
            "|    loss                 | 0.71        |\n",
            "|    n_updates            | 1390        |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    value_loss           | 1.26        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3568750, episode_reward=5.00 +/- 2.28\n",
            "Episode length: 907.80 +/- 202.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 908      |\n",
            "|    mean_reward     | 5        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3568750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3575000, episode_reward=5.20 +/- 2.56\n",
            "Episode length: 834.60 +/- 145.07\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 835      |\n",
            "|    mean_reward     | 5.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3575000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3581250, episode_reward=5.00 +/- 2.28\n",
            "Episode length: 907.60 +/- 202.95\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 908      |\n",
            "|    mean_reward     | 5        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3581250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 465      |\n",
            "|    ep_rew_mean     | 6.38     |\n",
            "| time/              |          |\n",
            "|    fps             | 373      |\n",
            "|    iterations      | 140      |\n",
            "|    time_elapsed    | 9584     |\n",
            "|    total_timesteps | 3584000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3587500, episode_reward=4.80 +/- 0.40\n",
            "Episode length: 750.80 +/- 9.60\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 751         |\n",
            "|    mean_reward          | 4.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3587500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013907655 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.866       |\n",
            "|    learning_rate        | 4.88e-05    |\n",
            "|    loss                 | 0.404       |\n",
            "|    n_updates            | 1400        |\n",
            "|    policy_gradient_loss | -0.0138     |\n",
            "|    value_loss           | 1.5         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3593750, episode_reward=4.60 +/- 0.49\n",
            "Episode length: 750.80 +/- 9.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 751      |\n",
            "|    mean_reward     | 4.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3593750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3600000, episode_reward=3.80 +/- 0.98\n",
            "Episode length: 713.20 +/- 65.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 713      |\n",
            "|    mean_reward     | 3.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3600000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3606250, episode_reward=3.60 +/- 1.36\n",
            "Episode length: 713.20 +/- 65.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 713      |\n",
            "|    mean_reward     | 3.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3606250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 452      |\n",
            "|    ep_rew_mean     | 5.89     |\n",
            "| time/              |          |\n",
            "|    fps             | 373      |\n",
            "|    iterations      | 141      |\n",
            "|    time_elapsed    | 9656     |\n",
            "|    total_timesteps | 3609600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3612500, episode_reward=5.80 +/- 1.47\n",
            "Episode length: 903.80 +/- 164.87\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 904         |\n",
            "|    mean_reward          | 5.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3612500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015053997 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.854       |\n",
            "|    learning_rate        | 4.84e-05    |\n",
            "|    loss                 | 1.3         |\n",
            "|    n_updates            | 1410        |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    value_loss           | 1.55        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3618750, episode_reward=5.20 +/- 1.47\n",
            "Episode length: 858.20 +/- 147.05\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 858      |\n",
            "|    mean_reward     | 5.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3618750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3625000, episode_reward=5.40 +/- 1.02\n",
            "Episode length: 908.40 +/- 160.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 908      |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3625000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3631250, episode_reward=6.00 +/- 1.67\n",
            "Episode length: 853.60 +/- 150.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 854      |\n",
            "|    mean_reward     | 6        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3631250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 462      |\n",
            "|    ep_rew_mean     | 5.96     |\n",
            "| time/              |          |\n",
            "|    fps             | 373      |\n",
            "|    iterations      | 142      |\n",
            "|    time_elapsed    | 9731     |\n",
            "|    total_timesteps | 3635200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3637500, episode_reward=4.20 +/- 0.40\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1.04e+03     |\n",
            "|    mean_reward          | 4.2          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 3637500      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0136111025 |\n",
            "|    clip_fraction        | 0.163        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.46        |\n",
            "|    explained_variance   | 0.869        |\n",
            "|    learning_rate        | 4.81e-05     |\n",
            "|    loss                 | 0.987        |\n",
            "|    n_updates            | 1420         |\n",
            "|    policy_gradient_loss | -0.0145      |\n",
            "|    value_loss           | 1.41         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=3643750, episode_reward=5.80 +/- 1.47\n",
            "Episode length: 960.80 +/- 94.55\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 961      |\n",
            "|    mean_reward     | 5.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3643750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3650000, episode_reward=5.40 +/- 0.80\n",
            "Episode length: 999.40 +/- 77.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 999      |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3650000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3656250, episode_reward=4.80 +/- 0.75\n",
            "Episode length: 1018.60 +/- 38.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 4.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3656250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 458      |\n",
            "|    ep_rew_mean     | 6.49     |\n",
            "| time/              |          |\n",
            "|    fps             | 373      |\n",
            "|    iterations      | 143      |\n",
            "|    time_elapsed    | 9811     |\n",
            "|    total_timesteps | 3660800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3662500, episode_reward=-5.00 +/- 0.00\n",
            "Episode length: 311.40 +/- 0.49\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 311        |\n",
            "|    mean_reward          | -5         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 3662500    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01518513 |\n",
            "|    clip_fraction        | 0.163      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.46      |\n",
            "|    explained_variance   | 0.875      |\n",
            "|    learning_rate        | 4.77e-05   |\n",
            "|    loss                 | 0.546      |\n",
            "|    n_updates            | 1430       |\n",
            "|    policy_gradient_loss | -0.014     |\n",
            "|    value_loss           | 1.39       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=3668750, episode_reward=-5.00 +/- 0.00\n",
            "Episode length: 311.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 311      |\n",
            "|    mean_reward     | -5       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3668750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3675000, episode_reward=-3.60 +/- 2.80\n",
            "Episode length: 378.60 +/- 134.70\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 379      |\n",
            "|    mean_reward     | -3.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3675000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3681250, episode_reward=-5.00 +/- 0.00\n",
            "Episode length: 311.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 311      |\n",
            "|    mean_reward     | -5       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3681250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 466      |\n",
            "|    ep_rew_mean     | 6.86     |\n",
            "| time/              |          |\n",
            "|    fps             | 373      |\n",
            "|    iterations      | 144      |\n",
            "|    time_elapsed    | 9866     |\n",
            "|    total_timesteps | 3686400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3687500, episode_reward=14.40 +/- 2.24\n",
            "Episode length: 1000.80 +/- 74.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1e+03       |\n",
            "|    mean_reward          | 14.4        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3687500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017172761 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 4.73e-05    |\n",
            "|    loss                 | 0.155       |\n",
            "|    n_updates            | 1440        |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    value_loss           | 1.44        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3693750, episode_reward=11.60 +/- 5.24\n",
            "Episode length: 886.60 +/- 196.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 887      |\n",
            "|    mean_reward     | 11.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3693750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3700000, episode_reward=15.60 +/- 0.49\n",
            "Episode length: 1041.40 +/- 6.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 15.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3700000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3706250, episode_reward=16.00 +/- 0.00\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 16       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3706250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 473      |\n",
            "|    ep_rew_mean     | 6.23     |\n",
            "| time/              |          |\n",
            "|    fps             | 373      |\n",
            "|    iterations      | 145      |\n",
            "|    time_elapsed    | 9947     |\n",
            "|    total_timesteps | 3712000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3712500, episode_reward=7.60 +/- 4.76\n",
            "Episode length: 908.40 +/- 140.32\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 908         |\n",
            "|    mean_reward          | 7.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3712500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015605402 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.88        |\n",
            "|    learning_rate        | 4.7e-05     |\n",
            "|    loss                 | 0.0826      |\n",
            "|    n_updates            | 1450        |\n",
            "|    policy_gradient_loss | -0.0159     |\n",
            "|    value_loss           | 1.27        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3718750, episode_reward=9.60 +/- 7.03\n",
            "Episode length: 914.80 +/- 150.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 915      |\n",
            "|    mean_reward     | 9.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3718750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3725000, episode_reward=12.60 +/- 5.82\n",
            "Episode length: 976.40 +/- 123.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 976      |\n",
            "|    mean_reward     | 12.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3725000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3731250, episode_reward=3.20 +/- 3.92\n",
            "Episode length: 782.00 +/- 104.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 782      |\n",
            "|    mean_reward     | 3.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3731250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3737500, episode_reward=13.40 +/- 3.72\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 13.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3737500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 484      |\n",
            "|    ep_rew_mean     | 6.35     |\n",
            "| time/              |          |\n",
            "|    fps             | 372      |\n",
            "|    iterations      | 146      |\n",
            "|    time_elapsed    | 10034    |\n",
            "|    total_timesteps | 3737600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3743750, episode_reward=4.60 +/- 4.50\n",
            "Episode length: 837.20 +/- 260.49\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 837         |\n",
            "|    mean_reward          | 4.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3743750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014842453 |\n",
            "|    clip_fraction        | 0.172       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.858       |\n",
            "|    learning_rate        | 4.66e-05    |\n",
            "|    loss                 | 0.951       |\n",
            "|    n_updates            | 1460        |\n",
            "|    policy_gradient_loss | -0.0141     |\n",
            "|    value_loss           | 1.63        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3750000, episode_reward=9.60 +/- 4.50\n",
            "Episode length: 963.80 +/- 111.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 964      |\n",
            "|    mean_reward     | 9.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3750000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3756250, episode_reward=5.40 +/- 2.87\n",
            "Episode length: 849.80 +/- 137.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 850      |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3756250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3762500, episode_reward=2.60 +/- 3.07\n",
            "Episode length: 716.40 +/- 217.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 716      |\n",
            "|    mean_reward     | 2.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3762500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 477      |\n",
            "|    ep_rew_mean     | 6.33     |\n",
            "| time/              |          |\n",
            "|    fps             | 372      |\n",
            "|    iterations      | 147      |\n",
            "|    time_elapsed    | 10108    |\n",
            "|    total_timesteps | 3763200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3768750, episode_reward=8.80 +/- 2.71\n",
            "Episode length: 892.40 +/- 138.39\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 892         |\n",
            "|    mean_reward          | 8.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3768750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015157505 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.839       |\n",
            "|    learning_rate        | 4.62e-05    |\n",
            "|    loss                 | 2.16        |\n",
            "|    n_updates            | 1470        |\n",
            "|    policy_gradient_loss | -0.015      |\n",
            "|    value_loss           | 1.57        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3775000, episode_reward=7.60 +/- 2.80\n",
            "Episode length: 928.00 +/- 148.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 928      |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3775000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3781250, episode_reward=5.80 +/- 3.25\n",
            "Episode length: 881.40 +/- 173.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 881      |\n",
            "|    mean_reward     | 5.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3781250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3787500, episode_reward=8.80 +/- 2.71\n",
            "Episode length: 918.40 +/- 142.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 918      |\n",
            "|    mean_reward     | 8.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3787500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 463      |\n",
            "|    ep_rew_mean     | 6.11     |\n",
            "| time/              |          |\n",
            "|    fps             | 371      |\n",
            "|    iterations      | 148      |\n",
            "|    time_elapsed    | 10186    |\n",
            "|    total_timesteps | 3788800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3793750, episode_reward=5.40 +/- 0.49\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.04e+03    |\n",
            "|    mean_reward          | 5.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3793750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014513306 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.839       |\n",
            "|    learning_rate        | 4.59e-05    |\n",
            "|    loss                 | 0.301       |\n",
            "|    n_updates            | 1480        |\n",
            "|    policy_gradient_loss | -0.0147     |\n",
            "|    value_loss           | 1.69        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3800000, episode_reward=3.80 +/- 3.43\n",
            "Episode length: 896.60 +/- 282.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 897      |\n",
            "|    mean_reward     | 3.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3800000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3806250, episode_reward=7.60 +/- 2.80\n",
            "Episode length: 1018.80 +/- 23.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3806250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3812500, episode_reward=10.40 +/- 4.27\n",
            "Episode length: 948.40 +/- 91.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 948      |\n",
            "|    mean_reward     | 10.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3812500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 463      |\n",
            "|    ep_rew_mean     | 6.12     |\n",
            "| time/              |          |\n",
            "|    fps             | 371      |\n",
            "|    iterations      | 149      |\n",
            "|    time_elapsed    | 10267    |\n",
            "|    total_timesteps | 3814400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3818750, episode_reward=8.00 +/- 4.56\n",
            "Episode length: 940.80 +/- 203.01\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 941         |\n",
            "|    mean_reward          | 8           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3818750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013636148 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.812       |\n",
            "|    learning_rate        | 4.55e-05    |\n",
            "|    loss                 | 0.815       |\n",
            "|    n_updates            | 1490        |\n",
            "|    policy_gradient_loss | -0.0136     |\n",
            "|    value_loss           | 1.76        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3825000, episode_reward=7.60 +/- 2.80\n",
            "Episode length: 1018.80 +/- 23.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3825000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3831250, episode_reward=5.60 +/- 0.49\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3831250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3837500, episode_reward=10.40 +/- 3.32\n",
            "Episode length: 821.00 +/- 234.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 821      |\n",
            "|    mean_reward     | 10.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3837500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 474      |\n",
            "|    ep_rew_mean     | 7.33     |\n",
            "| time/              |          |\n",
            "|    fps             | 371      |\n",
            "|    iterations      | 150      |\n",
            "|    time_elapsed    | 10345    |\n",
            "|    total_timesteps | 3840000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3843750, episode_reward=7.40 +/- 6.50\n",
            "Episode length: 811.40 +/- 123.30\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 811         |\n",
            "|    mean_reward          | 7.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3843750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015109525 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.868       |\n",
            "|    learning_rate        | 4.51e-05    |\n",
            "|    loss                 | 2.92        |\n",
            "|    n_updates            | 1500        |\n",
            "|    policy_gradient_loss | -0.0144     |\n",
            "|    value_loss           | 1.3         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3850000, episode_reward=5.00 +/- 4.90\n",
            "Episode length: 834.00 +/- 127.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 834      |\n",
            "|    mean_reward     | 5        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3850000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3856250, episode_reward=11.80 +/- 0.75\n",
            "Episode length: 853.20 +/- 111.70\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 853      |\n",
            "|    mean_reward     | 11.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3856250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3862500, episode_reward=5.20 +/- 5.15\n",
            "Episode length: 788.40 +/- 101.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 788      |\n",
            "|    mean_reward     | 5.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3862500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 481      |\n",
            "|    ep_rew_mean     | 7.73     |\n",
            "| time/              |          |\n",
            "|    fps             | 370      |\n",
            "|    iterations      | 151      |\n",
            "|    time_elapsed    | 10420    |\n",
            "|    total_timesteps | 3865600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3868750, episode_reward=2.20 +/- 0.40\n",
            "Episode length: 990.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 990         |\n",
            "|    mean_reward          | 2.2         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3868750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014404885 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.83        |\n",
            "|    learning_rate        | 4.48e-05    |\n",
            "|    loss                 | 1.59        |\n",
            "|    n_updates            | 1510        |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    value_loss           | 1.76        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3875000, episode_reward=2.20 +/- 0.40\n",
            "Episode length: 990.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 990      |\n",
            "|    mean_reward     | 2.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3875000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3881250, episode_reward=2.40 +/- 0.49\n",
            "Episode length: 990.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 990      |\n",
            "|    mean_reward     | 2.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3881250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3887500, episode_reward=2.00 +/- 0.00\n",
            "Episode length: 954.20 +/- 71.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 954      |\n",
            "|    mean_reward     | 2        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3887500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 489      |\n",
            "|    ep_rew_mean     | 6.85     |\n",
            "| time/              |          |\n",
            "|    fps             | 370      |\n",
            "|    iterations      | 152      |\n",
            "|    time_elapsed    | 10497    |\n",
            "|    total_timesteps | 3891200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3893750, episode_reward=8.00 +/- 2.00\n",
            "Episode length: 896.20 +/- 109.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 896         |\n",
            "|    mean_reward          | 8           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3893750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014808424 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.44       |\n",
            "|    explained_variance   | 0.873       |\n",
            "|    learning_rate        | 4.44e-05    |\n",
            "|    loss                 | 0.18        |\n",
            "|    n_updates            | 1520        |\n",
            "|    policy_gradient_loss | -0.0138     |\n",
            "|    value_loss           | 1.27        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3900000, episode_reward=8.00 +/- 2.00\n",
            "Episode length: 915.60 +/- 123.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 916      |\n",
            "|    mean_reward     | 8        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3900000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3906250, episode_reward=8.20 +/- 1.94\n",
            "Episode length: 899.60 +/- 111.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 900      |\n",
            "|    mean_reward     | 8.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3906250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3912500, episode_reward=8.00 +/- 2.00\n",
            "Episode length: 955.20 +/- 24.67\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 955      |\n",
            "|    mean_reward     | 8        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3912500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 508      |\n",
            "|    ep_rew_mean     | 7.39     |\n",
            "| time/              |          |\n",
            "|    fps             | 370      |\n",
            "|    iterations      | 153      |\n",
            "|    time_elapsed    | 10573    |\n",
            "|    total_timesteps | 3916800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3918750, episode_reward=6.40 +/- 1.74\n",
            "Episode length: 796.40 +/- 129.20\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 796          |\n",
            "|    mean_reward          | 6.4          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 3918750      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0139657995 |\n",
            "|    clip_fraction        | 0.161        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.45        |\n",
            "|    explained_variance   | 0.868        |\n",
            "|    learning_rate        | 4.4e-05      |\n",
            "|    loss                 | 0.228        |\n",
            "|    n_updates            | 1530         |\n",
            "|    policy_gradient_loss | -0.0151      |\n",
            "|    value_loss           | 1.49         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=3925000, episode_reward=7.60 +/- 1.20\n",
            "Episode length: 838.00 +/- 46.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 838      |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3925000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3931250, episode_reward=7.60 +/- 1.20\n",
            "Episode length: 838.00 +/- 46.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 838      |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3931250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3937500, episode_reward=5.40 +/- 3.20\n",
            "Episode length: 816.20 +/- 89.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 816      |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3937500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 490      |\n",
            "|    ep_rew_mean     | 7.21     |\n",
            "| time/              |          |\n",
            "|    fps             | 370      |\n",
            "|    iterations      | 154      |\n",
            "|    time_elapsed    | 10645    |\n",
            "|    total_timesteps | 3942400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3943750, episode_reward=7.00 +/- 4.65\n",
            "Episode length: 915.40 +/- 98.11\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 915         |\n",
            "|    mean_reward          | 7           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3943750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014009636 |\n",
            "|    clip_fraction        | 0.166       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.843       |\n",
            "|    learning_rate        | 4.37e-05    |\n",
            "|    loss                 | 0.683       |\n",
            "|    n_updates            | 1540        |\n",
            "|    policy_gradient_loss | -0.0153     |\n",
            "|    value_loss           | 1.53        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3950000, episode_reward=5.60 +/- 2.06\n",
            "Episode length: 786.40 +/- 162.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 786      |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3950000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3956250, episode_reward=12.40 +/- 5.39\n",
            "Episode length: 921.20 +/- 208.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 921      |\n",
            "|    mean_reward     | 12.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3956250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3962500, episode_reward=12.80 +/- 3.97\n",
            "Episode length: 880.00 +/- 229.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 880      |\n",
            "|    mean_reward     | 12.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3962500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 472      |\n",
            "|    ep_rew_mean     | 6.33     |\n",
            "| time/              |          |\n",
            "|    fps             | 370      |\n",
            "|    iterations      | 155      |\n",
            "|    time_elapsed    | 10722    |\n",
            "|    total_timesteps | 3968000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3968750, episode_reward=5.00 +/- 0.63\n",
            "Episode length: 702.80 +/- 86.40\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 703         |\n",
            "|    mean_reward          | 5           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 3968750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013855572 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.841       |\n",
            "|    learning_rate        | 4.33e-05    |\n",
            "|    loss                 | 0.867       |\n",
            "|    n_updates            | 1550        |\n",
            "|    policy_gradient_loss | -0.0136     |\n",
            "|    value_loss           | 1.64        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3975000, episode_reward=4.40 +/- 0.49\n",
            "Episode length: 746.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 746      |\n",
            "|    mean_reward     | 4.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3975000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3981250, episode_reward=4.40 +/- 0.49\n",
            "Episode length: 746.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 746      |\n",
            "|    mean_reward     | 4.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3981250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3987500, episode_reward=4.00 +/- 0.00\n",
            "Episode length: 781.80 +/- 71.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 782      |\n",
            "|    mean_reward     | 4        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3987500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 481      |\n",
            "|    ep_rew_mean     | 6.69     |\n",
            "| time/              |          |\n",
            "|    fps             | 370      |\n",
            "|    iterations      | 156      |\n",
            "|    time_elapsed    | 10792    |\n",
            "|    total_timesteps | 3993600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3993750, episode_reward=1.40 +/- 0.80\n",
            "Episode length: 746.20 +/- 32.40\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 746        |\n",
            "|    mean_reward          | 1.4        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 3993750    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01471405 |\n",
            "|    clip_fraction        | 0.172      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.46      |\n",
            "|    explained_variance   | 0.85       |\n",
            "|    learning_rate        | 4.29e-05   |\n",
            "|    loss                 | 0.723      |\n",
            "|    n_updates            | 1560       |\n",
            "|    policy_gradient_loss | -0.0149    |\n",
            "|    value_loss           | 1.52       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=4000000, episode_reward=3.00 +/- 4.05\n",
            "Episode length: 734.80 +/- 44.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 735      |\n",
            "|    mean_reward     | 3        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4006250, episode_reward=1.20 +/- 0.98\n",
            "Episode length: 762.40 +/- 39.68\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 762      |\n",
            "|    mean_reward     | 1.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4006250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4012500, episode_reward=0.80 +/- 0.75\n",
            "Episode length: 762.40 +/- 39.68\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 762      |\n",
            "|    mean_reward     | 0.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4012500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4018750, episode_reward=3.20 +/- 3.92\n",
            "Episode length: 753.00 +/- 46.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 753      |\n",
            "|    mean_reward     | 3.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4018750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 487      |\n",
            "|    ep_rew_mean     | 7.36     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 157      |\n",
            "|    time_elapsed    | 10869    |\n",
            "|    total_timesteps | 4019200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4025000, episode_reward=6.40 +/- 7.86\n",
            "Episode length: 908.60 +/- 119.54\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 909         |\n",
            "|    mean_reward          | 6.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4025000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014898832 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.46       |\n",
            "|    explained_variance   | 0.868       |\n",
            "|    learning_rate        | 4.26e-05    |\n",
            "|    loss                 | 0.742       |\n",
            "|    n_updates            | 1570        |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    value_loss           | 1.31        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4031250, episode_reward=12.00 +/- 7.51\n",
            "Episode length: 971.60 +/- 166.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 972      |\n",
            "|    mean_reward     | 12       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4031250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4037500, episode_reward=12.80 +/- 6.43\n",
            "Episode length: 1006.20 +/- 97.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.01e+03 |\n",
            "|    mean_reward     | 12.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4037500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4043750, episode_reward=15.40 +/- 0.49\n",
            "Episode length: 1055.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.06e+03 |\n",
            "|    mean_reward     | 15.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4043750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 491      |\n",
            "|    ep_rew_mean     | 7.75     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 158      |\n",
            "|    time_elapsed    | 10948    |\n",
            "|    total_timesteps | 4044800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4050000, episode_reward=6.60 +/- 6.34\n",
            "Episode length: 711.80 +/- 41.92\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 712         |\n",
            "|    mean_reward          | 6.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4050000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014854807 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.851       |\n",
            "|    learning_rate        | 4.22e-05    |\n",
            "|    loss                 | 0.946       |\n",
            "|    n_updates            | 1580        |\n",
            "|    policy_gradient_loss | -0.0135     |\n",
            "|    value_loss           | 1.42        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4056250, episode_reward=3.00 +/- 1.79\n",
            "Episode length: 742.40 +/- 47.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 742      |\n",
            "|    mean_reward     | 3        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4056250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4062500, episode_reward=5.20 +/- 3.87\n",
            "Episode length: 807.80 +/- 94.51\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 808      |\n",
            "|    mean_reward     | 5.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4062500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4068750, episode_reward=2.00 +/- 2.45\n",
            "Episode length: 785.00 +/- 31.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 785      |\n",
            "|    mean_reward     | 2        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4068750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 487      |\n",
            "|    ep_rew_mean     | 7.67     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 159      |\n",
            "|    time_elapsed    | 11018    |\n",
            "|    total_timesteps | 4070400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4075000, episode_reward=8.00 +/- 4.10\n",
            "Episode length: 712.80 +/- 40.66\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 713         |\n",
            "|    mean_reward          | 8           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4075000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014743789 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.886       |\n",
            "|    learning_rate        | 4.19e-05    |\n",
            "|    loss                 | 0.251       |\n",
            "|    n_updates            | 1590        |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 1.28        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4081250, episode_reward=6.20 +/- 3.43\n",
            "Episode length: 729.40 +/- 33.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 729      |\n",
            "|    mean_reward     | 6.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4081250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4087500, episode_reward=9.00 +/- 5.25\n",
            "Episode length: 732.80 +/- 35.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 733      |\n",
            "|    mean_reward     | 9        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4087500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4093750, episode_reward=6.20 +/- 1.47\n",
            "Episode length: 617.20 +/- 105.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 617      |\n",
            "|    mean_reward     | 6.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4093750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 503      |\n",
            "|    ep_rew_mean     | 8.24     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 160      |\n",
            "|    time_elapsed    | 11087    |\n",
            "|    total_timesteps | 4096000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4100000, episode_reward=12.40 +/- 6.95\n",
            "Episode length: 646.20 +/- 143.35\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 646         |\n",
            "|    mean_reward          | 12.4        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4100000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014630735 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0.875       |\n",
            "|    learning_rate        | 4.15e-05    |\n",
            "|    loss                 | 0.304       |\n",
            "|    n_updates            | 1600        |\n",
            "|    policy_gradient_loss | -0.0116     |\n",
            "|    value_loss           | 1.35        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4106250, episode_reward=7.60 +/- 5.71\n",
            "Episode length: 710.80 +/- 70.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 711      |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4106250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4112500, episode_reward=13.40 +/- 7.31\n",
            "Episode length: 704.80 +/- 117.39\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 705      |\n",
            "|    mean_reward     | 13.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4112500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4118750, episode_reward=7.20 +/- 5.42\n",
            "Episode length: 691.20 +/- 109.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 691      |\n",
            "|    mean_reward     | 7.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4118750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 487      |\n",
            "|    ep_rew_mean     | 7.38     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 161      |\n",
            "|    time_elapsed    | 11155    |\n",
            "|    total_timesteps | 4121600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4125000, episode_reward=5.40 +/- 3.88\n",
            "Episode length: 714.00 +/- 178.61\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 714        |\n",
            "|    mean_reward          | 5.4        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 4125000    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01447653 |\n",
            "|    clip_fraction        | 0.157      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.4       |\n",
            "|    explained_variance   | 0.868      |\n",
            "|    learning_rate        | 4.11e-05   |\n",
            "|    loss                 | 0.368      |\n",
            "|    n_updates            | 1610       |\n",
            "|    policy_gradient_loss | -0.0132    |\n",
            "|    value_loss           | 1.52       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=4131250, episode_reward=1.80 +/- 5.49\n",
            "Episode length: 569.20 +/- 80.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 569      |\n",
            "|    mean_reward     | 1.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4131250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4137500, episode_reward=10.00 +/- 3.52\n",
            "Episode length: 636.40 +/- 192.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 636      |\n",
            "|    mean_reward     | 10       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4137500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4143750, episode_reward=8.60 +/- 0.49\n",
            "Episode length: 633.20 +/- 194.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 633      |\n",
            "|    mean_reward     | 8.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4143750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 454      |\n",
            "|    ep_rew_mean     | 6.72     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 162      |\n",
            "|    time_elapsed    | 11222    |\n",
            "|    total_timesteps | 4147200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4150000, episode_reward=4.40 +/- 8.66\n",
            "Episode length: 869.40 +/- 140.80\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 869          |\n",
            "|    mean_reward          | 4.4          |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 4150000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0137343025 |\n",
            "|    clip_fraction        | 0.157        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | 0.86         |\n",
            "|    learning_rate        | 4.08e-05     |\n",
            "|    loss                 | 0.896        |\n",
            "|    n_updates            | 1620         |\n",
            "|    policy_gradient_loss | -0.0148      |\n",
            "|    value_loss           | 1.54         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=4156250, episode_reward=2.40 +/- 7.28\n",
            "Episode length: 869.40 +/- 140.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 869      |\n",
            "|    mean_reward     | 2.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4156250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4162500, episode_reward=13.20 +/- 4.62\n",
            "Episode length: 993.80 +/- 88.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 994      |\n",
            "|    mean_reward     | 13.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4162500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4168750, episode_reward=11.80 +/- 7.41\n",
            "Episode length: 976.40 +/- 123.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 976      |\n",
            "|    mean_reward     | 11.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4168750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 463      |\n",
            "|    ep_rew_mean     | 6.89     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 163      |\n",
            "|    time_elapsed    | 11297    |\n",
            "|    total_timesteps | 4172800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4175000, episode_reward=11.80 +/- 8.40\n",
            "Episode length: 963.60 +/- 116.80\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 964         |\n",
            "|    mean_reward          | 11.8        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4175000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015898118 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.44       |\n",
            "|    explained_variance   | 0.886       |\n",
            "|    learning_rate        | 4.04e-05    |\n",
            "|    loss                 | 0.303       |\n",
            "|    n_updates            | 1630        |\n",
            "|    policy_gradient_loss | -0.0151     |\n",
            "|    value_loss           | 1.32        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4181250, episode_reward=12.40 +/- 8.71\n",
            "Episode length: 963.60 +/- 116.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 964      |\n",
            "|    mean_reward     | 12.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4181250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4187500, episode_reward=12.00 +/- 8.51\n",
            "Episode length: 963.60 +/- 116.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 964      |\n",
            "|    mean_reward     | 12       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4187500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4193750, episode_reward=17.70 +/- 1.94\n",
            "Episode length: 1038.60 +/- 33.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 17.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4193750  |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 473      |\n",
            "|    ep_rew_mean     | 7.89     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 164      |\n",
            "|    time_elapsed    | 11376    |\n",
            "|    total_timesteps | 4198400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4200000, episode_reward=7.80 +/- 3.12\n",
            "Episode length: 759.00 +/- 63.69\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 759         |\n",
            "|    mean_reward          | 7.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4200000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013530426 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.44       |\n",
            "|    explained_variance   | 0.891       |\n",
            "|    learning_rate        | 4e-05       |\n",
            "|    loss                 | 0.181       |\n",
            "|    n_updates            | 1640        |\n",
            "|    policy_gradient_loss | -0.0146     |\n",
            "|    value_loss           | 1.33        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4206250, episode_reward=9.20 +/- 2.64\n",
            "Episode length: 785.00 +/- 52.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 785      |\n",
            "|    mean_reward     | 9.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4206250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4212500, episode_reward=9.20 +/- 2.64\n",
            "Episode length: 785.00 +/- 52.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 785      |\n",
            "|    mean_reward     | 9.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4212500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4218750, episode_reward=7.80 +/- 3.12\n",
            "Episode length: 759.00 +/- 63.69\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 759      |\n",
            "|    mean_reward     | 7.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4218750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 469      |\n",
            "|    ep_rew_mean     | 7.62     |\n",
            "| time/              |          |\n",
            "|    fps             | 369      |\n",
            "|    iterations      | 165      |\n",
            "|    time_elapsed    | 11446    |\n",
            "|    total_timesteps | 4224000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4225000, episode_reward=4.40 +/- 0.49\n",
            "Episode length: 843.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 843         |\n",
            "|    mean_reward          | 4.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4225000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015672639 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0.868       |\n",
            "|    learning_rate        | 3.97e-05    |\n",
            "|    loss                 | 0.938       |\n",
            "|    n_updates            | 1650        |\n",
            "|    policy_gradient_loss | -0.0144     |\n",
            "|    value_loss           | 1.49        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4231250, episode_reward=4.40 +/- 0.49\n",
            "Episode length: 843.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 843      |\n",
            "|    mean_reward     | 4.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4231250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4237500, episode_reward=4.20 +/- 0.40\n",
            "Episode length: 843.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 843      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4237500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4243750, episode_reward=4.20 +/- 0.40\n",
            "Episode length: 843.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 843      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4243750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 474      |\n",
            "|    ep_rew_mean     | 7.7      |\n",
            "| time/              |          |\n",
            "|    fps             | 368      |\n",
            "|    iterations      | 166      |\n",
            "|    time_elapsed    | 11521    |\n",
            "|    total_timesteps | 4249600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4250000, episode_reward=8.40 +/- 5.39\n",
            "Episode length: 785.00 +/- 127.37\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 785         |\n",
            "|    mean_reward          | 8.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4250000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014941662 |\n",
            "|    clip_fraction        | 0.172       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0.908       |\n",
            "|    learning_rate        | 3.93e-05    |\n",
            "|    loss                 | 0.787       |\n",
            "|    n_updates            | 1660        |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    value_loss           | 1.16        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4256250, episode_reward=8.60 +/- 5.24\n",
            "Episode length: 908.40 +/- 119.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 908      |\n",
            "|    mean_reward     | 8.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4256250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4262500, episode_reward=8.60 +/- 5.24\n",
            "Episode length: 856.40 +/- 147.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 856      |\n",
            "|    mean_reward     | 8.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4262500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4268750, episode_reward=10.80 +/- 5.15\n",
            "Episode length: 908.40 +/- 119.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 908      |\n",
            "|    mean_reward     | 10.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4268750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4275000, episode_reward=12.80 +/- 4.40\n",
            "Episode length: 960.40 +/- 38.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 960      |\n",
            "|    mean_reward     | 12.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4275000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 487      |\n",
            "|    ep_rew_mean     | 8.14     |\n",
            "| time/              |          |\n",
            "|    fps             | 368      |\n",
            "|    iterations      | 167      |\n",
            "|    time_elapsed    | 11603    |\n",
            "|    total_timesteps | 4275200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4281250, episode_reward=6.40 +/- 3.83\n",
            "Episode length: 895.20 +/- 174.89\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 895         |\n",
            "|    mean_reward          | 6.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4281250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014383403 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.45       |\n",
            "|    explained_variance   | 0.9         |\n",
            "|    learning_rate        | 3.89e-05    |\n",
            "|    loss                 | 0.136       |\n",
            "|    n_updates            | 1670        |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    value_loss           | 1.16        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4287500, episode_reward=7.40 +/- 4.32\n",
            "Episode length: 1041.40 +/- 6.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 7.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4287500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4293750, episode_reward=9.60 +/- 4.45\n",
            "Episode length: 1041.40 +/- 6.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 9.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4293750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4300000, episode_reward=7.00 +/- 4.52\n",
            "Episode length: 970.00 +/- 144.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 970      |\n",
            "|    mean_reward     | 7        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4300000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 467      |\n",
            "|    ep_rew_mean     | 6.92     |\n",
            "| time/              |          |\n",
            "|    fps             | 368      |\n",
            "|    iterations      | 168      |\n",
            "|    time_elapsed    | 11682    |\n",
            "|    total_timesteps | 4300800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4306250, episode_reward=10.20 +/- 5.08\n",
            "Episode length: 895.20 +/- 174.89\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 895         |\n",
            "|    mean_reward          | 10.2        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4306250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014654552 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.864       |\n",
            "|    learning_rate        | 3.86e-05    |\n",
            "|    loss                 | 0.84        |\n",
            "|    n_updates            | 1680        |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    value_loss           | 1.56        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4312500, episode_reward=12.20 +/- 4.12\n",
            "Episode length: 966.60 +/- 142.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 967      |\n",
            "|    mean_reward     | 12.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4312500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4318750, episode_reward=10.40 +/- 5.24\n",
            "Episode length: 895.20 +/- 174.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 895      |\n",
            "|    mean_reward     | 10.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4318750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4325000, episode_reward=14.20 +/- 0.40\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 14.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4325000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 453      |\n",
            "|    ep_rew_mean     | 6.17     |\n",
            "| time/              |          |\n",
            "|    fps             | 367      |\n",
            "|    iterations      | 169      |\n",
            "|    time_elapsed    | 11759    |\n",
            "|    total_timesteps | 4326400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4331250, episode_reward=15.00 +/- 0.00\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.04e+03    |\n",
            "|    mean_reward          | 15          |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4331250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013871975 |\n",
            "|    clip_fraction        | 0.166       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 3.82e-05    |\n",
            "|    loss                 | 0.257       |\n",
            "|    n_updates            | 1690        |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 1.32        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4337500, episode_reward=14.40 +/- 1.20\n",
            "Episode length: 921.60 +/- 142.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 922      |\n",
            "|    mean_reward     | 14.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4337500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4343750, episode_reward=15.00 +/- 0.00\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 15       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4343750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4350000, episode_reward=12.00 +/- 6.00\n",
            "Episode length: 953.80 +/- 168.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 954      |\n",
            "|    mean_reward     | 12       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4350000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 470      |\n",
            "|    ep_rew_mean     | 7.63     |\n",
            "| time/              |          |\n",
            "|    fps             | 367      |\n",
            "|    iterations      | 170      |\n",
            "|    time_elapsed    | 11837    |\n",
            "|    total_timesteps | 4352000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4356250, episode_reward=11.20 +/- 6.68\n",
            "Episode length: 909.20 +/- 208.60\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 909         |\n",
            "|    mean_reward          | 11.2        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4356250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015849644 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.904       |\n",
            "|    learning_rate        | 3.78e-05    |\n",
            "|    loss                 | 0.784       |\n",
            "|    n_updates            | 1700        |\n",
            "|    policy_gradient_loss | -0.0157     |\n",
            "|    value_loss           | 1.12        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4362500, episode_reward=13.20 +/- 6.62\n",
            "Episode length: 948.00 +/- 214.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 948      |\n",
            "|    mean_reward     | 13.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4362500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4368750, episode_reward=14.40 +/- 3.72\n",
            "Episode length: 1016.20 +/- 77.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 14.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4368750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4375000, episode_reward=15.70 +/- 3.74\n",
            "Episode length: 843.40 +/- 174.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 843      |\n",
            "|    mean_reward     | 15.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4375000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 470      |\n",
            "|    ep_rew_mean     | 8.02     |\n",
            "| time/              |          |\n",
            "|    fps             | 367      |\n",
            "|    iterations      | 171      |\n",
            "|    time_elapsed    | 11915    |\n",
            "|    total_timesteps | 4377600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4381250, episode_reward=2.80 +/- 2.71\n",
            "Episode length: 762.60 +/- 337.29\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 763         |\n",
            "|    mean_reward          | 2.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4381250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014882181 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.893       |\n",
            "|    learning_rate        | 3.75e-05    |\n",
            "|    loss                 | 0.762       |\n",
            "|    n_updates            | 1710        |\n",
            "|    policy_gradient_loss | -0.0139     |\n",
            "|    value_loss           | 1.29        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4387500, episode_reward=5.50 +/- 6.03\n",
            "Episode length: 741.20 +/- 322.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 741      |\n",
            "|    mean_reward     | 5.5      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4387500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4393750, episode_reward=3.70 +/- 4.33\n",
            "Episode length: 756.00 +/- 326.68\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 756      |\n",
            "|    mean_reward     | 3.7      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4393750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4400000, episode_reward=2.80 +/- 2.71\n",
            "Episode length: 762.60 +/- 337.29\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 763      |\n",
            "|    mean_reward     | 2.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4400000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 442      |\n",
            "|    ep_rew_mean     | 6.32     |\n",
            "| time/              |          |\n",
            "|    fps             | 367      |\n",
            "|    iterations      | 172      |\n",
            "|    time_elapsed    | 11985    |\n",
            "|    total_timesteps | 4403200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4406250, episode_reward=8.80 +/- 7.60\n",
            "Episode length: 764.60 +/- 334.85\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 765         |\n",
            "|    mean_reward          | 8.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4406250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013970282 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.859       |\n",
            "|    learning_rate        | 3.71e-05    |\n",
            "|    loss                 | 1.56        |\n",
            "|    n_updates            | 1720        |\n",
            "|    policy_gradient_loss | -0.0151     |\n",
            "|    value_loss           | 1.59        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4412500, episode_reward=12.20 +/- 6.62\n",
            "Episode length: 788.00 +/- 251.21\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 788      |\n",
            "|    mean_reward     | 12.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4412500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4418750, episode_reward=12.00 +/- 6.00\n",
            "Episode length: 901.40 +/- 273.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 901      |\n",
            "|    mean_reward     | 12       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4418750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4425000, episode_reward=8.80 +/- 7.60\n",
            "Episode length: 708.00 +/- 306.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 708      |\n",
            "|    mean_reward     | 8.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4425000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 453      |\n",
            "|    ep_rew_mean     | 6.04     |\n",
            "| time/              |          |\n",
            "|    fps             | 367      |\n",
            "|    iterations      | 173      |\n",
            "|    time_elapsed    | 12056    |\n",
            "|    total_timesteps | 4428800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4431250, episode_reward=16.10 +/- 2.20\n",
            "Episode length: 1070.60 +/- 65.20\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.07e+03    |\n",
            "|    mean_reward          | 16.1        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4431250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015160634 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0.889       |\n",
            "|    learning_rate        | 3.67e-05    |\n",
            "|    loss                 | 1.54        |\n",
            "|    n_updates            | 1730        |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    value_loss           | 1.43        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4437500, episode_reward=10.60 +/- 5.39\n",
            "Episode length: 895.20 +/- 174.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 895      |\n",
            "|    mean_reward     | 10.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4437500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4443750, episode_reward=14.10 +/- 5.44\n",
            "Episode length: 992.60 +/- 163.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 993      |\n",
            "|    mean_reward     | 14.1     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4443750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4450000, episode_reward=13.00 +/- 4.52\n",
            "Episode length: 966.60 +/- 142.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 967      |\n",
            "|    mean_reward     | 13       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4450000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 477      |\n",
            "|    ep_rew_mean     | 7.53     |\n",
            "| time/              |          |\n",
            "|    fps             | 367      |\n",
            "|    iterations      | 174      |\n",
            "|    time_elapsed    | 12135    |\n",
            "|    total_timesteps | 4454400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4456250, episode_reward=11.00 +/- 8.51\n",
            "Episode length: 966.60 +/- 142.80\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 967          |\n",
            "|    mean_reward          | 11           |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 4456250      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0152511215 |\n",
            "|    clip_fraction        | 0.166        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.9          |\n",
            "|    learning_rate        | 3.64e-05     |\n",
            "|    loss                 | 0.708        |\n",
            "|    n_updates            | 1740         |\n",
            "|    policy_gradient_loss | -0.0139      |\n",
            "|    value_loss           | 1.14         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=4462500, episode_reward=15.40 +/- 0.49\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 15.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4462500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4468750, episode_reward=6.80 +/- 10.46\n",
            "Episode length: 895.20 +/- 174.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 895      |\n",
            "|    mean_reward     | 6.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4468750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4475000, episode_reward=11.00 +/- 8.51\n",
            "Episode length: 966.60 +/- 142.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 967      |\n",
            "|    mean_reward     | 11       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4475000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 481      |\n",
            "|    ep_rew_mean     | 7.46     |\n",
            "| time/              |          |\n",
            "|    fps             | 366      |\n",
            "|    iterations      | 175      |\n",
            "|    time_elapsed    | 12214    |\n",
            "|    total_timesteps | 4480000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4481250, episode_reward=13.00 +/- 9.51\n",
            "Episode length: 953.80 +/- 136.40\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 954        |\n",
            "|    mean_reward          | 13         |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 4481250    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01432483 |\n",
            "|    clip_fraction        | 0.161      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.44      |\n",
            "|    explained_variance   | 0.868      |\n",
            "|    learning_rate        | 3.6e-05    |\n",
            "|    loss                 | 0.818      |\n",
            "|    n_updates            | 1750       |\n",
            "|    policy_gradient_loss | -0.015     |\n",
            "|    value_loss           | 1.55       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=4487500, episode_reward=13.00 +/- 9.53\n",
            "Episode length: 885.80 +/- 166.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 886      |\n",
            "|    mean_reward     | 13       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4487500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4493750, episode_reward=13.00 +/- 9.51\n",
            "Episode length: 953.80 +/- 136.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 954      |\n",
            "|    mean_reward     | 13       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4493750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4500000, episode_reward=7.80 +/- 11.27\n",
            "Episode length: 885.60 +/- 167.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 886      |\n",
            "|    mean_reward     | 7.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4500000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 478      |\n",
            "|    ep_rew_mean     | 7.1      |\n",
            "| time/              |          |\n",
            "|    fps             | 366      |\n",
            "|    iterations      | 176      |\n",
            "|    time_elapsed    | 12288    |\n",
            "|    total_timesteps | 4505600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4506250, episode_reward=0.40 +/- 8.71\n",
            "Episode length: 663.40 +/- 35.20\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 663        |\n",
            "|    mean_reward          | 0.4        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 4506250    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01575304 |\n",
            "|    clip_fraction        | 0.169      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.37      |\n",
            "|    explained_variance   | 0.887      |\n",
            "|    learning_rate        | 3.56e-05   |\n",
            "|    loss                 | 0.276      |\n",
            "|    n_updates            | 1760       |\n",
            "|    policy_gradient_loss | -0.0137    |\n",
            "|    value_loss           | 1.32       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=4512500, episode_reward=7.20 +/- 3.92\n",
            "Episode length: 925.40 +/- 174.28\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 925      |\n",
            "|    mean_reward     | 7.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4512500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4518750, episode_reward=3.00 +/- 4.52\n",
            "Episode length: 966.60 +/- 142.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 967      |\n",
            "|    mean_reward     | 3        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4518750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4525000, episode_reward=3.00 +/- 4.52\n",
            "Episode length: 966.60 +/- 142.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 967      |\n",
            "|    mean_reward     | 3        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4525000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 483      |\n",
            "|    ep_rew_mean     | 7.81     |\n",
            "| time/              |          |\n",
            "|    fps             | 366      |\n",
            "|    iterations      | 177      |\n",
            "|    time_elapsed    | 12364    |\n",
            "|    total_timesteps | 4531200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4531250, episode_reward=4.60 +/- 0.49\n",
            "Episode length: 846.00 +/- 235.15\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 846         |\n",
            "|    mean_reward          | 4.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4531250     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014477554 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.874       |\n",
            "|    learning_rate        | 3.53e-05    |\n",
            "|    loss                 | 0.916       |\n",
            "|    n_updates            | 1770        |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    value_loss           | 1.37        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4537500, episode_reward=5.20 +/- 0.40\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 5.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4537500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4543750, episode_reward=4.80 +/- 0.40\n",
            "Episode length: 942.00 +/- 192.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 942      |\n",
            "|    mean_reward     | 4.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4543750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4550000, episode_reward=6.60 +/- 2.24\n",
            "Episode length: 958.60 +/- 158.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 959      |\n",
            "|    mean_reward     | 6.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4550000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4556250, episode_reward=9.40 +/- 5.46\n",
            "Episode length: 929.20 +/- 134.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 929      |\n",
            "|    mean_reward     | 9.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4556250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 483      |\n",
            "|    ep_rew_mean     | 7.71     |\n",
            "| time/              |          |\n",
            "|    fps             | 366      |\n",
            "|    iterations      | 178      |\n",
            "|    time_elapsed    | 12449    |\n",
            "|    total_timesteps | 4556800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4562500, episode_reward=3.40 +/- 3.20\n",
            "Episode length: 809.20 +/- 280.22\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 809         |\n",
            "|    mean_reward          | 3.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4562500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015122996 |\n",
            "|    clip_fraction        | 0.167       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0.869       |\n",
            "|    learning_rate        | 3.49e-05    |\n",
            "|    loss                 | 0.177       |\n",
            "|    n_updates            | 1780        |\n",
            "|    policy_gradient_loss | -0.0165     |\n",
            "|    value_loss           | 1.47        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4568750, episode_reward=5.40 +/- 0.49\n",
            "Episode length: 1038.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4568750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4575000, episode_reward=7.60 +/- 5.46\n",
            "Episode length: 753.20 +/- 238.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 753      |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4575000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4581250, episode_reward=8.60 +/- 3.61\n",
            "Episode length: 867.60 +/- 208.70\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 868      |\n",
            "|    mean_reward     | 8.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4581250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 477      |\n",
            "|    ep_rew_mean     | 6.98     |\n",
            "| time/              |          |\n",
            "|    fps             | 365      |\n",
            "|    iterations      | 179      |\n",
            "|    time_elapsed    | 12523    |\n",
            "|    total_timesteps | 4582400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4587500, episode_reward=12.80 +/- 3.66\n",
            "Episode length: 828.20 +/- 178.97\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 828         |\n",
            "|    mean_reward          | 12.8        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4587500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013995845 |\n",
            "|    clip_fraction        | 0.153       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.871       |\n",
            "|    learning_rate        | 3.45e-05    |\n",
            "|    loss                 | 0.238       |\n",
            "|    n_updates            | 1790        |\n",
            "|    policy_gradient_loss | -0.0151     |\n",
            "|    value_loss           | 1.49        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4593750, episode_reward=15.60 +/- 0.80\n",
            "Episode length: 883.20 +/- 83.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 883      |\n",
            "|    mean_reward     | 15.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4593750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4600000, episode_reward=14.00 +/- 4.00\n",
            "Episode length: 928.20 +/- 6.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 928      |\n",
            "|    mean_reward     | 14       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4600000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4606250, episode_reward=14.60 +/- 3.83\n",
            "Episode length: 928.20 +/- 6.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 928      |\n",
            "|    mean_reward     | 14.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4606250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 478      |\n",
            "|    ep_rew_mean     | 7.37     |\n",
            "| time/              |          |\n",
            "|    fps             | 365      |\n",
            "|    iterations      | 180      |\n",
            "|    time_elapsed    | 12598    |\n",
            "|    total_timesteps | 4608000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4612500, episode_reward=9.00 +/- 5.02\n",
            "Episode length: 970.40 +/- 169.20\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 970        |\n",
            "|    mean_reward          | 9          |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 4612500    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01417902 |\n",
            "|    clip_fraction        | 0.145      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.41      |\n",
            "|    explained_variance   | 0.881      |\n",
            "|    learning_rate        | 3.42e-05   |\n",
            "|    loss                 | 0.379      |\n",
            "|    n_updates            | 1800       |\n",
            "|    policy_gradient_loss | -0.0149    |\n",
            "|    value_loss           | 1.41       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=4618750, episode_reward=8.80 +/- 4.62\n",
            "Episode length: 993.40 +/- 123.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 993      |\n",
            "|    mean_reward     | 8.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4618750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4625000, episode_reward=6.40 +/- 0.49\n",
            "Episode length: 976.20 +/- 96.51\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 976      |\n",
            "|    mean_reward     | 6.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4625000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4631250, episode_reward=6.60 +/- 0.49\n",
            "Episode length: 1051.60 +/- 6.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.05e+03 |\n",
            "|    mean_reward     | 6.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4631250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 467      |\n",
            "|    ep_rew_mean     | 7.19     |\n",
            "| time/              |          |\n",
            "|    fps             | 365      |\n",
            "|    iterations      | 181      |\n",
            "|    time_elapsed    | 12677    |\n",
            "|    total_timesteps | 4633600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4637500, episode_reward=4.40 +/- 1.74\n",
            "Episode length: 940.40 +/- 133.56\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 940         |\n",
            "|    mean_reward          | 4.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4637500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013750431 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.41       |\n",
            "|    explained_variance   | 0.86        |\n",
            "|    learning_rate        | 3.38e-05    |\n",
            "|    loss                 | 0.5         |\n",
            "|    n_updates            | 1810        |\n",
            "|    policy_gradient_loss | -0.0169     |\n",
            "|    value_loss           | 1.54        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4643750, episode_reward=4.20 +/- 1.83\n",
            "Episode length: 940.40 +/- 133.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 940      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4643750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4650000, episode_reward=4.60 +/- 1.36\n",
            "Episode length: 943.80 +/- 136.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 944      |\n",
            "|    mean_reward     | 4.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4650000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4656250, episode_reward=10.20 +/- 5.23\n",
            "Episode length: 993.20 +/- 115.29\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 993      |\n",
            "|    mean_reward     | 10.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4656250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 473      |\n",
            "|    ep_rew_mean     | 7.63     |\n",
            "| time/              |          |\n",
            "|    fps             | 365      |\n",
            "|    iterations      | 182      |\n",
            "|    time_elapsed    | 12756    |\n",
            "|    total_timesteps | 4659200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4662500, episode_reward=13.40 +/- 6.89\n",
            "Episode length: 1064.80 +/- 43.85\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.06e+03    |\n",
            "|    mean_reward          | 13.4        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4662500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013652692 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0.884       |\n",
            "|    learning_rate        | 3.34e-05    |\n",
            "|    loss                 | 0.517       |\n",
            "|    n_updates            | 1820        |\n",
            "|    policy_gradient_loss | -0.0157     |\n",
            "|    value_loss           | 1.43        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4668750, episode_reward=10.20 +/- 6.85\n",
            "Episode length: 1049.00 +/- 105.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.05e+03 |\n",
            "|    mean_reward     | 10.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4668750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4675000, episode_reward=6.00 +/- 6.10\n",
            "Episode length: 957.60 +/- 117.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 958      |\n",
            "|    mean_reward     | 6        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4675000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4681250, episode_reward=15.40 +/- 6.22\n",
            "Episode length: 1074.20 +/- 111.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.07e+03 |\n",
            "|    mean_reward     | 15.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4681250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 494      |\n",
            "|    ep_rew_mean     | 8        |\n",
            "| time/              |          |\n",
            "|    fps             | 364      |\n",
            "|    iterations      | 183      |\n",
            "|    time_elapsed    | 12836    |\n",
            "|    total_timesteps | 4684800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4687500, episode_reward=3.60 +/- 1.62\n",
            "Episode length: 920.40 +/- 116.16\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 920         |\n",
            "|    mean_reward          | 3.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4687500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014185268 |\n",
            "|    clip_fraction        | 0.153       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 3.31e-05    |\n",
            "|    loss                 | 0.12        |\n",
            "|    n_updates            | 1830        |\n",
            "|    policy_gradient_loss | -0.0165     |\n",
            "|    value_loss           | 1.36        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4693750, episode_reward=7.80 +/- 5.11\n",
            "Episode length: 951.00 +/- 199.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 951      |\n",
            "|    mean_reward     | 7.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4693750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4700000, episode_reward=5.60 +/- 0.49\n",
            "Episode length: 1048.20 +/- 8.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.05e+03 |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4700000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4706250, episode_reward=5.20 +/- 0.40\n",
            "Episode length: 1044.80 +/- 8.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.04e+03 |\n",
            "|    mean_reward     | 5.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4706250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 502      |\n",
            "|    ep_rew_mean     | 8.23     |\n",
            "| time/              |          |\n",
            "|    fps             | 364      |\n",
            "|    iterations      | 184      |\n",
            "|    time_elapsed    | 12914    |\n",
            "|    total_timesteps | 4710400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4712500, episode_reward=5.00 +/- 2.10\n",
            "Episode length: 801.00 +/- 31.84\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 801        |\n",
            "|    mean_reward          | 5          |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 4712500    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01279353 |\n",
            "|    clip_fraction        | 0.146      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.39      |\n",
            "|    explained_variance   | 0.884      |\n",
            "|    learning_rate        | 3.27e-05   |\n",
            "|    loss                 | 1.31       |\n",
            "|    n_updates            | 1840       |\n",
            "|    policy_gradient_loss | -0.0127    |\n",
            "|    value_loss           | 1.25       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=4718750, episode_reward=6.80 +/- 0.40\n",
            "Episode length: 827.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 827      |\n",
            "|    mean_reward     | 6.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4718750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4725000, episode_reward=5.60 +/- 1.85\n",
            "Episode length: 730.20 +/- 163.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 730      |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4725000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4731250, episode_reward=5.80 +/- 1.94\n",
            "Episode length: 814.00 +/- 26.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 814      |\n",
            "|    mean_reward     | 5.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4731250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 487      |\n",
            "|    ep_rew_mean     | 8.26     |\n",
            "| time/              |          |\n",
            "|    fps             | 364      |\n",
            "|    iterations      | 185      |\n",
            "|    time_elapsed    | 12988    |\n",
            "|    total_timesteps | 4736000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4737500, episode_reward=10.20 +/- 4.12\n",
            "Episode length: 836.60 +/- 103.10\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 837         |\n",
            "|    mean_reward          | 10.2        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4737500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012626975 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.873       |\n",
            "|    learning_rate        | 3.23e-05    |\n",
            "|    loss                 | 0.902       |\n",
            "|    n_updates            | 1850        |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    value_loss           | 1.42        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4743750, episode_reward=10.60 +/- 3.93\n",
            "Episode length: 781.40 +/- 24.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 781      |\n",
            "|    mean_reward     | 10.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4743750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4750000, episode_reward=9.00 +/- 4.00\n",
            "Episode length: 771.80 +/- 19.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 772      |\n",
            "|    mean_reward     | 9        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4750000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4756250, episode_reward=13.20 +/- 4.45\n",
            "Episode length: 826.40 +/- 107.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 826      |\n",
            "|    mean_reward     | 13.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4756250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 470      |\n",
            "|    ep_rew_mean     | 7.25     |\n",
            "| time/              |          |\n",
            "|    fps             | 364      |\n",
            "|    iterations      | 186      |\n",
            "|    time_elapsed    | 13060    |\n",
            "|    total_timesteps | 4761600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4762500, episode_reward=8.00 +/- 2.53\n",
            "Episode length: 905.20 +/- 86.09\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 905         |\n",
            "|    mean_reward          | 8           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4762500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013521867 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0.857       |\n",
            "|    learning_rate        | 3.2e-05     |\n",
            "|    loss                 | 0.782       |\n",
            "|    n_updates            | 1860        |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 1.66        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4768750, episode_reward=8.80 +/- 2.04\n",
            "Episode length: 834.80 +/- 92.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 835      |\n",
            "|    mean_reward     | 8.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4768750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4775000, episode_reward=7.20 +/- 1.17\n",
            "Episode length: 870.20 +/- 124.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 870      |\n",
            "|    mean_reward     | 7.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4775000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4781250, episode_reward=7.60 +/- 1.74\n",
            "Episode length: 905.40 +/- 47.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 905      |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4781250  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 481      |\n",
            "|    ep_rew_mean     | 7.39     |\n",
            "| time/              |          |\n",
            "|    fps             | 364      |\n",
            "|    iterations      | 187      |\n",
            "|    time_elapsed    | 13134    |\n",
            "|    total_timesteps | 4787200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4787500, episode_reward=4.60 +/- 2.15\n",
            "Episode length: 937.80 +/- 143.54\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 938         |\n",
            "|    mean_reward          | 4.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4787500     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013454075 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | 0.879       |\n",
            "|    learning_rate        | 3.16e-05    |\n",
            "|    loss                 | 0.541       |\n",
            "|    n_updates            | 1870        |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    value_loss           | 1.4         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4793750, episode_reward=5.40 +/- 1.74\n",
            "Episode length: 996.40 +/- 117.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 996      |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4793750  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4800000, episode_reward=4.60 +/- 2.15\n",
            "Episode length: 937.80 +/- 143.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 938      |\n",
            "|    mean_reward     | 4.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4800000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4806250, episode_reward=6.20 +/- 1.94\n",
            "Episode length: 953.40 +/- 126.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 953      |\n",
            "|    mean_reward     | 6.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4806250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4812500, episode_reward=5.60 +/- 1.36\n",
            "Episode length: 891.40 +/- 213.38\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 891      |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4812500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 498      |\n",
            "|    ep_rew_mean     | 8.74     |\n",
            "| time/              |          |\n",
            "|    fps             | 364      |\n",
            "|    iterations      | 188      |\n",
            "|    time_elapsed    | 13219    |\n",
            "|    total_timesteps | 4812800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4818750, episode_reward=8.00 +/- 1.55\n",
            "Episode length: 897.40 +/- 70.43\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 897         |\n",
            "|    mean_reward          | 8           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4818750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014478214 |\n",
            "|    clip_fraction        | 0.157       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.35       |\n",
            "|    explained_variance   | 0.898       |\n",
            "|    learning_rate        | 3.12e-05    |\n",
            "|    loss                 | 0.175       |\n",
            "|    n_updates            | 1880        |\n",
            "|    policy_gradient_loss | -0.0152     |\n",
            "|    value_loss           | 1.16        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4825000, episode_reward=6.80 +/- 6.68\n",
            "Episode length: 800.80 +/- 47.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 801      |\n",
            "|    mean_reward     | 6.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4825000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4831250, episode_reward=8.20 +/- 1.60\n",
            "Episode length: 892.00 +/- 73.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 892      |\n",
            "|    mean_reward     | 8.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4831250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4837500, episode_reward=8.00 +/- 1.55\n",
            "Episode length: 897.20 +/- 70.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 897      |\n",
            "|    mean_reward     | 8        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4837500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 506      |\n",
            "|    ep_rew_mean     | 8.62     |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 189      |\n",
            "|    time_elapsed    | 13295    |\n",
            "|    total_timesteps | 4838400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4843750, episode_reward=11.00 +/- 5.33\n",
            "Episode length: 870.40 +/- 192.29\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 870         |\n",
            "|    mean_reward          | 11          |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4843750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012388043 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.876       |\n",
            "|    learning_rate        | 3.09e-05    |\n",
            "|    loss                 | 0.452       |\n",
            "|    n_updates            | 1890        |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    value_loss           | 1.46        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4850000, episode_reward=6.00 +/- 0.63\n",
            "Episode length: 1028.40 +/- 7.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.03e+03 |\n",
            "|    mean_reward     | 6        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4850000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4856250, episode_reward=6.40 +/- 0.49\n",
            "Episode length: 1025.20 +/- 6.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.03e+03 |\n",
            "|    mean_reward     | 6.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4856250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4862500, episode_reward=5.60 +/- 1.50\n",
            "Episode length: 973.20 +/- 105.78\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 973      |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4862500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 489      |\n",
            "|    ep_rew_mean     | 7.49     |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 190      |\n",
            "|    time_elapsed    | 13374    |\n",
            "|    total_timesteps | 4864000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4868750, episode_reward=11.40 +/- 4.67\n",
            "Episode length: 812.20 +/- 139.70\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 812         |\n",
            "|    mean_reward          | 11.4        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4868750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013838674 |\n",
            "|    clip_fraction        | 0.149       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.849       |\n",
            "|    learning_rate        | 3.05e-05    |\n",
            "|    loss                 | 0.756       |\n",
            "|    n_updates            | 1900        |\n",
            "|    policy_gradient_loss | -0.016      |\n",
            "|    value_loss           | 1.54        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4875000, episode_reward=8.60 +/- 0.49\n",
            "Episode length: 845.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 845      |\n",
            "|    mean_reward     | 8.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4875000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4881250, episode_reward=10.60 +/- 3.20\n",
            "Episode length: 803.00 +/- 84.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 803      |\n",
            "|    mean_reward     | 10.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4881250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4887500, episode_reward=9.40 +/- 4.03\n",
            "Episode length: 841.60 +/- 127.51\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 842      |\n",
            "|    mean_reward     | 9.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4887500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 482      |\n",
            "|    ep_rew_mean     | 7.54     |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 191      |\n",
            "|    time_elapsed    | 13447    |\n",
            "|    total_timesteps | 4889600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4893750, episode_reward=5.00 +/- 2.10\n",
            "Episode length: 947.60 +/- 132.45\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 948         |\n",
            "|    mean_reward          | 5           |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4893750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013772457 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.34       |\n",
            "|    explained_variance   | 0.877       |\n",
            "|    learning_rate        | 3.01e-05    |\n",
            "|    loss                 | 0.441       |\n",
            "|    n_updates            | 1910        |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    value_loss           | 1.32        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4900000, episode_reward=5.00 +/- 2.10\n",
            "Episode length: 947.60 +/- 132.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 948      |\n",
            "|    mean_reward     | 5        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4900000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4906250, episode_reward=4.20 +/- 2.04\n",
            "Episode length: 889.00 +/- 136.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 889      |\n",
            "|    mean_reward     | 4.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4906250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4912500, episode_reward=5.00 +/- 2.10\n",
            "Episode length: 957.40 +/- 119.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 957      |\n",
            "|    mean_reward     | 5        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4912500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 511      |\n",
            "|    ep_rew_mean     | 9.01     |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 192      |\n",
            "|    time_elapsed    | 13524    |\n",
            "|    total_timesteps | 4915200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4918750, episode_reward=5.80 +/- 1.94\n",
            "Episode length: 997.00 +/- 116.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 997         |\n",
            "|    mean_reward          | 5.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4918750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013931111 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | 0.9         |\n",
            "|    learning_rate        | 2.98e-05    |\n",
            "|    loss                 | 0.788       |\n",
            "|    n_updates            | 1920        |\n",
            "|    policy_gradient_loss | -0.0137     |\n",
            "|    value_loss           | 1.09        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=4925000, episode_reward=6.00 +/- 0.00\n",
            "Episode length: 1055.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.06e+03 |\n",
            "|    mean_reward     | 6        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4925000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4931250, episode_reward=5.40 +/- 1.74\n",
            "Episode length: 997.00 +/- 116.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 997      |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4931250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4937500, episode_reward=5.60 +/- 1.85\n",
            "Episode length: 997.00 +/- 116.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 997      |\n",
            "|    mean_reward     | 5.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4937500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 509      |\n",
            "|    ep_rew_mean     | 8.7      |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 193      |\n",
            "|    time_elapsed    | 13604    |\n",
            "|    total_timesteps | 4940800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4943750, episode_reward=14.40 +/- 4.22\n",
            "Episode length: 785.20 +/- 126.40\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 785          |\n",
            "|    mean_reward          | 14.4         |\n",
            "| time/                   |              |\n",
            "|    total_timesteps      | 4943750      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0129727945 |\n",
            "|    clip_fraction        | 0.137        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | 0.874        |\n",
            "|    learning_rate        | 2.94e-05     |\n",
            "|    loss                 | 1.65         |\n",
            "|    n_updates            | 1930         |\n",
            "|    policy_gradient_loss | -0.0155      |\n",
            "|    value_loss           | 1.5          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=4950000, episode_reward=12.20 +/- 6.43\n",
            "Episode length: 774.20 +/- 139.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 774      |\n",
            "|    mean_reward     | 12.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4950000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4956250, episode_reward=16.40 +/- 0.49\n",
            "Episode length: 722.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 722      |\n",
            "|    mean_reward     | 16.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4956250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4962500, episode_reward=8.00 +/- 7.35\n",
            "Episode length: 746.00 +/- 19.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 746      |\n",
            "|    mean_reward     | 8        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4962500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 516      |\n",
            "|    ep_rew_mean     | 8.12     |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 194      |\n",
            "|    time_elapsed    | 13675    |\n",
            "|    total_timesteps | 4966400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4968750, episode_reward=6.00 +/- 3.29\n",
            "Episode length: 602.20 +/- 132.07\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 602        |\n",
            "|    mean_reward          | 6          |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 4968750    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01394174 |\n",
            "|    clip_fraction        | 0.151      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.36      |\n",
            "|    explained_variance   | 0.906      |\n",
            "|    learning_rate        | 2.91e-05   |\n",
            "|    loss                 | 0.244      |\n",
            "|    n_updates            | 1940       |\n",
            "|    policy_gradient_loss | -0.0152    |\n",
            "|    value_loss           | 0.99       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=4975000, episode_reward=6.00 +/- 3.29\n",
            "Episode length: 602.20 +/- 132.07\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 602      |\n",
            "|    mean_reward     | 6        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4975000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4981250, episode_reward=10.80 +/- 5.84\n",
            "Episode length: 613.00 +/- 98.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 613      |\n",
            "|    mean_reward     | 10.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4981250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4987500, episode_reward=8.40 +/- 4.45\n",
            "Episode length: 589.80 +/- 118.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 590      |\n",
            "|    mean_reward     | 8.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4987500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 542      |\n",
            "|    ep_rew_mean     | 9.31     |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 195      |\n",
            "|    time_elapsed    | 13740    |\n",
            "|    total_timesteps | 4992000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4993750, episode_reward=6.80 +/- 1.17\n",
            "Episode length: 957.40 +/- 195.20\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 957        |\n",
            "|    mean_reward          | 6.8        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 4993750    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01319031 |\n",
            "|    clip_fraction        | 0.14       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.34      |\n",
            "|    explained_variance   | 0.906      |\n",
            "|    learning_rate        | 2.87e-05   |\n",
            "|    loss                 | 0.104      |\n",
            "|    n_updates            | 1950       |\n",
            "|    policy_gradient_loss | -0.0149    |\n",
            "|    value_loss           | 1.08       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=5000000, episode_reward=6.60 +/- 2.58\n",
            "Episode length: 820.40 +/- 208.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 820      |\n",
            "|    mean_reward     | 6.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5006250, episode_reward=7.20 +/- 0.98\n",
            "Episode length: 960.60 +/- 188.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 961      |\n",
            "|    mean_reward     | 7.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5006250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5012500, episode_reward=4.00 +/- 2.45\n",
            "Episode length: 876.40 +/- 157.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 876      |\n",
            "|    mean_reward     | 4        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5012500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 513      |\n",
            "|    ep_rew_mean     | 8.02     |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 196      |\n",
            "|    time_elapsed    | 13815    |\n",
            "|    total_timesteps | 5017600  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5018750, episode_reward=7.40 +/- 4.63\n",
            "Episode length: 942.00 +/- 145.79\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 942         |\n",
            "|    mean_reward          | 7.4         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 5018750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013947675 |\n",
            "|    clip_fraction        | 0.144       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0.863       |\n",
            "|    learning_rate        | 2.83e-05    |\n",
            "|    loss                 | 0.123       |\n",
            "|    n_updates            | 1960        |\n",
            "|    policy_gradient_loss | -0.015      |\n",
            "|    value_loss           | 1.52        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=5025000, episode_reward=3.60 +/- 1.96\n",
            "Episode length: 915.40 +/- 114.66\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 915      |\n",
            "|    mean_reward     | 3.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5025000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5031250, episode_reward=5.80 +/- 1.94\n",
            "Episode length: 1006.20 +/- 97.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.01e+03 |\n",
            "|    mean_reward     | 5.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5031250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5037500, episode_reward=7.20 +/- 4.66\n",
            "Episode length: 935.20 +/- 150.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 935      |\n",
            "|    mean_reward     | 7.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5037500  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 495      |\n",
            "|    ep_rew_mean     | 7.13     |\n",
            "| time/              |          |\n",
            "|    fps             | 363      |\n",
            "|    iterations      | 197      |\n",
            "|    time_elapsed    | 13892    |\n",
            "|    total_timesteps | 5043200  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5043750, episode_reward=4.80 +/- 2.32\n",
            "Episode length: 902.20 +/- 131.64\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 902         |\n",
            "|    mean_reward          | 4.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 5043750     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013033748 |\n",
            "|    clip_fraction        | 0.136       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | 0.881       |\n",
            "|    learning_rate        | 2.8e-05     |\n",
            "|    loss                 | 0.519       |\n",
            "|    n_updates            | 1970        |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    value_loss           | 1.34        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=5050000, episode_reward=6.80 +/- 0.75\n",
            "Episode length: 938.40 +/- 159.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 938      |\n",
            "|    mean_reward     | 6.8      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5050000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5056250, episode_reward=8.20 +/- 5.71\n",
            "Episode length: 918.80 +/- 117.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 919      |\n",
            "|    mean_reward     | 8.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5056250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5062500, episode_reward=6.20 +/- 2.14\n",
            "Episode length: 895.80 +/- 205.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 896      |\n",
            "|    mean_reward     | 6.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5062500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5068750, episode_reward=5.40 +/- 1.74\n",
            "Episode length: 996.40 +/- 117.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 996      |\n",
            "|    mean_reward     | 5.4      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5068750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 517      |\n",
            "|    ep_rew_mean     | 8.97     |\n",
            "| time/              |          |\n",
            "|    fps             | 362      |\n",
            "|    iterations      | 198      |\n",
            "|    time_elapsed    | 13977    |\n",
            "|    total_timesteps | 5068800  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5075000, episode_reward=7.80 +/- 2.14\n",
            "Episode length: 971.80 +/- 103.65\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 972         |\n",
            "|    mean_reward          | 7.8         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 5075000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013193242 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | 0.903       |\n",
            "|    learning_rate        | 2.76e-05    |\n",
            "|    loss                 | 0.456       |\n",
            "|    n_updates            | 1980        |\n",
            "|    policy_gradient_loss | -0.0142     |\n",
            "|    value_loss           | 1.02        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=5081250, episode_reward=6.00 +/- 0.00\n",
            "Episode length: 1019.40 +/- 71.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 6        |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5081250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5087500, episode_reward=7.60 +/- 2.73\n",
            "Episode length: 1007.40 +/- 95.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.01e+03 |\n",
            "|    mean_reward     | 7.6      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5087500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5093750, episode_reward=7.20 +/- 2.40\n",
            "Episode length: 1007.40 +/- 95.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.01e+03 |\n",
            "|    mean_reward     | 7.2      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5093750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 518      |\n",
            "|    ep_rew_mean     | 9.79     |\n",
            "| time/              |          |\n",
            "|    fps             | 362      |\n",
            "|    iterations      | 199      |\n",
            "|    time_elapsed    | 14056    |\n",
            "|    total_timesteps | 5094400  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5100000, episode_reward=6.60 +/- 0.49\n",
            "Episode length: 1055.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.06e+03    |\n",
            "|    mean_reward          | 6.6         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 5100000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013120537 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | 0.895       |\n",
            "|    learning_rate        | 2.72e-05    |\n",
            "|    loss                 | 0.409       |\n",
            "|    n_updates            | 1990        |\n",
            "|    policy_gradient_loss | -0.0138     |\n",
            "|    value_loss           | 1.2         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=5106250, episode_reward=13.60 +/- 5.85\n",
            "Episode length: 801.00 +/- 232.35\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 801      |\n",
            "|    mean_reward     | 13.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5106250  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5112500, episode_reward=14.20 +/- 5.91\n",
            "Episode length: 784.20 +/- 222.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 784      |\n",
            "|    mean_reward     | 14.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5112500  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5118750, episode_reward=11.20 +/- 6.05\n",
            "Episode length: 869.60 +/- 228.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 870      |\n",
            "|    mean_reward     | 11.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5118750  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 511      |\n",
            "|    ep_rew_mean     | 9.26     |\n",
            "| time/              |          |\n",
            "|    fps             | 362      |\n",
            "|    iterations      | 200      |\n",
            "|    time_elapsed    | 14131    |\n",
            "|    total_timesteps | 5120000  |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5125000, episode_reward=13.40 +/- 5.24\n",
            "Episode length: 712.40 +/- 24.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 712         |\n",
            "|    mean_reward          | 13.4        |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 5125000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012644586 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | 0.888       |\n",
            "|    learning_rate        | 2.69e-05    |\n",
            "|    loss                 | 0.727       |\n",
            "|    n_updates            | 2000        |\n",
            "|    policy_gradient_loss | -0.015      |\n",
            "|    value_loss           | 1.23        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=5131250, episode_reward=12.40 +/- 7.06\n",
            "Episode length: 775.00 +/- 75.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 775      |\n",
            "|    mean_reward     | 12.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5131250  |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c23c2ea2ff34>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7_000_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwandb_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    320\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    321\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     def reset(\n",
            "\u001b[0;32m/content/Q-Bert_RL/EnvironmentWrappers/ObsRewardWrapper.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_init_objects_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_objects_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhud\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    320\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    321\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ale_py/env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_truncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "gr2zvnYKJex1",
        "outputId": "c145dd6b-f4b2-4ac5-de1a-04850a781bc6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>▁▁███▅▅▅▅▅▄▅▃▅▆▆▅▃▅▅▄▅▅▇█▅▅▅▅▅██▇▇▆█▅▇▇▅</td></tr><tr><td>eval/mean_reward</td><td>▃▁▄▁▂▃▅▄▅▅▆▇▇█▅▄▅▄▅▅▇▅▅▄▅▅█▄▆▇▆▇▆█▆▅▆▅▇▅</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>rollout/ep_len_mean</td><td>▂▁▁▁▁▃▃▃▄▄▄▅▅▅▆▅▅▅▆▆▆▆▇▇▇▇▆▆▆▇▇▇▆▆▇▇▆▇██</td></tr><tr><td>rollout/ep_rew_mean</td><td>▂▁▂▂▂▁▁▂▂▃▄▄▅▅▆▆▆▆▅▆▆▆▆▇▇▆▆▇▇▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>time/fps</td><td>█▇▇▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/approx_kl</td><td>▃▂▂▁▂▆▅▅██▇█▆▇▆▆▇▇▇█▇▇██▆▆█▆▆▇▆▆▇▇▆▆▅▆▆▅</td></tr><tr><td>train/clip_fraction</td><td>▅▂▁▃▃▄▆▆▅▄▆▆▅▄▆▅██▅▇▆▇▅▅▆▇▇▅▅▇▇▅▅▆▅▅▄▃▄▄</td></tr><tr><td>train/clip_range</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy_loss</td><td>▂▁▁▁▁▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▅▄▄▅▅▅▅▅▅▅▆▅▇▆▇▇█</td></tr><tr><td>train/explained_variance</td><td>▁▅▆▇▆▆▆▆▅▇▆▇▇▇▇██▇██▇▇▇▇▇██▇▇█▇█▇██▇▇▇██</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▆▆▆▆▆▆▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/policy_gradient_loss</td><td>██▇▆▆▄▄▅▄▄▃▄▄▃▂▂▄▃▃▃▂▃▄▂▃▃▂▃▂▃▂▂▃▃▂▂▁▁▂▃</td></tr><tr><td>train/value_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>775</td></tr><tr><td>eval/mean_reward</td><td>12.4</td></tr><tr><td>global_step</td><td>5131250</td></tr><tr><td>rollout/ep_len_mean</td><td>511.3</td></tr><tr><td>rollout/ep_rew_mean</td><td>9.26</td></tr><tr><td>time/fps</td><td>362</td></tr><tr><td>train/approx_kl</td><td>0.01264</td></tr><tr><td>train/clip_fraction</td><td>0.13461</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>-1.31794</td></tr><tr><td>train/explained_variance</td><td>0.88811</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.72711</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.01496</td></tr><tr><td>train/value_loss</td><td>1.22985</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">jolly-resonance-14</strong> at: <a href='https://wandb.ai/Q-BertRLTeam/QBERT-RL/runs/vx73kc3x' target=\"_blank\">https://wandb.ai/Q-BertRLTeam/QBERT-RL/runs/vx73kc3x</a><br> View project at: <a href='https://wandb.ai/Q-BertRLTeam/QBERT-RL' target=\"_blank\">https://wandb.ai/Q-BertRLTeam/QBERT-RL</a><br>Synced 5 W&B file(s), 0 media file(s), 15 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250201_200015-vx73kc3x/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"ALE/Qbert-ram-v5\", render_mode=\"rgb_array\")\n",
        "env = ObsRewardWrapper(env)"
      ],
      "metadata": {
        "id": "oVRak-ScdWDv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DummyVecEnv per compatibilità con Stable-Baselines3\n",
        "env = DummyVecEnv([lambda: env])"
      ],
      "metadata": {
        "id": "lvg9Ckj1bLZj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Registra video\n",
        "video_folder = \"./videos/\"\n",
        "env = VecVideoRecorder(\n",
        "    env,               # Ambiente\n",
        "    video_folder,      # Cartella per salvare i video\n",
        "    record_video_trigger=lambda x: x % 10000000 == 0,  # Registra ogni 1000 passi\n",
        "    video_length=10000000 # Durata massima del video in passi\n",
        ")"
      ],
      "metadata": {
        "id": "Y4neEWrxbOCg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetta l'ambiente per registrare un episodio\n",
        "obs = env.reset()\n",
        "\n",
        "# Registra 3 episodi\n",
        "for episode in range(3):\n",
        "    obs = env.reset()\n",
        "    for _ in range(10000000):  # Durata massima dell'episodio\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        obs, rewards, dones, info = env.step(action)\n",
        "        if dones[0]:  # L'episodio è terminato\n",
        "            break\n",
        "\n",
        "env.close()  # Salva il video"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTWj4-mjbyPs",
        "outputId": "a508a273-0a6e-4e20-aa68-1223b51aa117"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/Q-Bert_RL/videos/rl-video-step-0-to-step-10000000.mp4.\n",
            "Moviepy - Writing video /content/Q-Bert_RL/videos/rl-video-step-0-to-step-10000000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/Q-Bert_RL/videos/rl-video-step-0-to-step-10000000.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/Q-Bert_RL/videos/rl-video-step-0-to-step-10000000.mp4.\n",
            "Moviepy - Writing video /content/Q-Bert_RL/videos/rl-video-step-0-to-step-10000000.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/Q-Bert_RL/videos/rl-video-step-0-to-step-10000000.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}